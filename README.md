![image](https://github.com/user-attachments/assets/83c618fb-e39d-44db-9b9a-b9d6509d6ef4)[![Review Assignment Due Date](https://classroom.github.com/assets/deadline-readme-button-22041afd0340ce965d47ae6ef1cefeee28c7c493a6346c4f15d667ab976d596c.svg)](https://classroom.github.com/a/Lj3YlzJp)
# Proyecto Final 2025-1: AI Neural Network
## **CS2013 ProgramaciÃ³n III** Â· Informe Final

### **DescripciÃ³n**

> Ejemplo: ImplementaciÃ³n de una red neuronal multicapa en C++ para clasificaciÃ³n de dÃ­gitos manuscritos.

### Contenidos

1. [Datos generales](#datos-generales)
2. [Requisitos e instalaciÃ³n](#requisitos-e-instalaciÃ³n)
3. [InvestigaciÃ³n teÃ³rica](#1-investigaciÃ³n-teÃ³rica)
4. [DiseÃ±o e implementaciÃ³n](#2-diseÃ±o-e-implementaciÃ³n)
5. [EjecuciÃ³n](#3-ejecuciÃ³n)
6. [AnÃ¡lisis del rendimiento](#4-anÃ¡lisis-del-rendimiento)
7. [Trabajo en equipo](#5-trabajo-en-equipo)
8. [Conclusiones](#6-conclusiones)
9. [BibliografÃ­a](#7-bibliografÃ­a)
10. [Licencia](#licencia)
---

### Datos generales

* **Tema**: Redes Neuronales en AI
* **Grupo**: `group_3_custom_name`
* **Integrantes**:

  * Henrry Andre Valle Enriquez â€“ 202310310 (Responsable de investigaciÃ³n teÃ³rica)
  * JosÃ© Mariano Llacta GonzÃ¡lez â€“ 202410365 (Desarrollo de la arquitectura)
  * Eliseo David Velasquez Diaz â€“ 202410184 (ImplementaciÃ³n del modelo)
  * Alejandro Vargas Rios â€“ 202410089 (Pruebas y benchmarking)
  * Alumno E â€“ 209900005 (DocumentaciÃ³n y demo)

> *Nota: Reemplazar nombres y roles reales.*

---

### Requisitos e instalaciÃ³n

1. **Compilador**: GCC 11 o superior
2. **Dependencias**:

   * CMake 3.18+
   * Eigen 3.4
   * \[Otra librerÃ­a opcional]
3. **InstalaciÃ³n**:

   ```bash
   git clone https://github.com/EJEMPLO/proyecto-final.git
   cd proyecto-final
   mkdir build && cd build
   cmake ..
   make
   ```

> *Ejemplo de repositorio y comandos, ajustar segÃºn proyecto.*

---

### 1. InvestigaciÃ³n teÃ³rica

* **Objetivo**: Explorar fundamentos y arquitecturas de redes neuronales.
* **Contenido de ejemplo**:

  1. Historia y evoluciÃ³n de las NNs.   

El desarrollo de las redes neuronales artificiales se remonta a mediados del siglo XX. En 1943, McCulloch y Pitts propusieron el primer modelo de neurona artificial (una funciÃ³n lÃ³gica umbral), sentando las bases del conexionismo. AÃ±os mÃ¡s tarde, el psicÃ³logo Donald Hebb formulÃ³ en 1949 la regla de aprendizaje que lleva su nombre, enfatizando que la fortaleza de las conexiones neuronales aumenta si ambas neuronas se activan simultÃ¡neamente. Un hito fundamental ocurriÃ³ en 1958, cuando Frank Rosenblatt creÃ³ el PerceptrÃ³n, considerado la primera neurona artificial entrenable.El perceptrÃ³n de Rosenblatt podÃ­a aprender a clasificar patrones simples ajustando pesos sinÃ¡pticos, lo que marcÃ³ el inicio del campo de aprendizaje automÃ¡tico con redes neuronales [1]. Sin embargo, a finales de la dÃ©cada de 1960, las expectativas sobre las redes neuronales sufrieron un revÃ©s. En 1969, Marvin Minsky y Seymour Papert publicaron una crÃ­tica que demostraba limitaciones del perceptrÃ³n de capa simple (por ejemplo, su incapacidad para resolver la funciÃ³n XOR), ademÃ¡s de seÃ±alar la insuficiencia del hardware de la Ã©poca para entrenar redes mÃ¡s complejas. Estas observaciones llevaron a un estancamiento en la investigaciÃ³n de redes neuronales durante varios aÃ±os, periodo a menudo denominado el â€œinvierno de la IAâ€.
El resurgimiento llegÃ³ en la dÃ©cada de 1980 gracias a la introducciÃ³n de redes neuronales de mÃºltiples capas y algoritmos de entrenamiento mÃ¡s eficientes. Un avance clave fue el algoritmo de retropropagaciÃ³n del error (backpropagation), inicialmente descrito por Paul Werbos en 1975 y popularizado en 1986 por Rumelhart, Hinton y Williams [2]. La retropropagaciÃ³n permitiÃ³ ajustar los pesos de redes con una o mÃ¡s capas ocultas propagando hacia atrÃ¡s el error de salida, haciendo factible entrenar los llamados perceptrones multicapa (MLP) de forma supervisada. A partir de entonces, se lograron Ã©xitos en tareas de reconocimiento de patrones que antes eran intratables para redes de una sola capa. Durante los aÃ±os 1990, otros mÃ©todos de aprendizaje automÃ¡tico como las mÃ¡quinas de soporte vectorial cobraron protagonismo, pero las redes neuronales mantuvieron su desarrollo en dominios especÃ­ficos. Ya en el siglo XXI, la combinaciÃ³n de algoritmos mejorados, grandes volÃºmenes de datos y un aumento notable en el poder de cÃ³mputo (especialmente con el uso de GPUs) propiciÃ³ el auge del aprendizaje profundo (deep learning). Modelos con muchas capas ocultas (redes neuronales profundas) comenzaron a superar el estado del arte en reconocimiento de imÃ¡genes, voz y texto alrededor de 2012, dando lugar a la era actual de la IA basada en redes neuronales [5]. En resumen, las redes neuronales han evolucionado desde perceptrones simples hasta arquitecturas profundas complejas, pasando por la etapa 
crucial de los MLP, que sentaron las bases conceptuales de muchos avances modernos.

### Fundamentos matemÃ¡ticos bÃ¡sicos

Las redes neuronales artificiales se inspiran en las neuronas biolÃ³gicas, pero se definen mediante modelos matemÃ¡ticos. La neurona artificial bÃ¡sica recibe una serie de entradas numÃ©ricas $x_1, x_2, \dots, x_n$, cada una asociada a un peso sinÃ¡ptico $w_1, w_2, \dots, w_n$ que representa la importancia de esa entrada. La neurona calcula primero una combinaciÃ³n lineal de sus entradas â€“ comÃºnmente denominada suma ponderada â€“ a la que se le agrega un tÃ©rmino llamado bias o sesgo ($b$). En tÃ©rminos matemÃ¡ticos, el potencial de activaciÃ³n de la neurona (a menudo denotado $z$) es:
z=w1x1+w2x2+â‹¯+wnxn+b.z = w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b.z=w1x1+w2x2+â‹¯+wnxn+b.
Este valor $z$ es entonces transformado por medio de una funciÃ³n de activaciÃ³n no lineal para producir la salida final de la neurona. La necesidad de esta funciÃ³n no lineal radica en que, si todas las neuronas aplicaran solo transformaciones lineales, incluso una red con mÃºltiples capas colapsarÃ­a algebraicamente en una sola capa equivalente (perdiendo capacidad de modelar relaciones complejas). Por tanto, las funciones de activaciÃ³n introducen no linealidad, permitiendo que la red pueda aproximar funciones y patrones arbitrariamente complejos en los datos [3]. Durante el proceso de aprendizaje, el objetivo es encontrar los valores de los pesos $w_{ij}$ y sesgos $b_j$ para cada neurona $j$ que minimicen el error en las predicciones de la red. Esto se logra definiendo una funciÃ³n de pÃ©rdida (por ejemplo, el error cuadrÃ¡tico medio o la entropÃ­a cruzada) que cuantifica la discrepancia entre la salida prevista por la red y la salida deseada, y luego ajustando los pesos para minimizar esa pÃ©rdida.
La minimizaciÃ³n de la funciÃ³n de pÃ©rdida tÃ­picamente se realiza mediante mÃ©todos de descenso por gradiente. En esencia, la red calcula el gradiente (derivada parcial) de la pÃ©rdida con respecto a cada peso â€“ informaciÃ³n que indica en quÃ© direcciÃ³n y cuÃ¡nto debe cambiar cada parÃ¡metro para reducir el error. El algoritmo de retropropagaciÃ³n es la tÃ©cnica que permite obtener estos gradientes de manera eficiente, aplicando la regla de la cadena del cÃ¡lculo diferencial a travÃ©s de las capas de la red. En la fase de propagaciÃ³n hacia adelante, se calcula la salida de la red para un conjunto de entradas; luego se evalÃºa la pÃ©rdida comparando con la salida esperada. En la fase de propagaciÃ³n hacia atrÃ¡s, ese error se propaga desde la capa de salida hacia las capas ocultas, distribuyendo a cada neurona una porciÃ³n de la responsabilidad del error total. MatemÃ¡ticamente, la retropropagaciÃ³n permite calcular el gradiente de la pÃ©rdida respecto a cada peso interno de la red, y con ello ajustar ligeramente cada peso en la direcciÃ³n que mÃ¡s reduce el error (paso dictado por el descenso de gradiente). Repetido este ciclo muchas veces con numerosos datos de entrenamiento, la red va aprendiendo: sus pesos convergen a valores que logran predicciones cada vez mÃ¡s precisas [2]. En sÃ­ntesis, el fundamento matemÃ¡tico de un MLP consiste en componer muchas funciones lineales y no lineales (neuronas) y optimizar sus parÃ¡metros mediante mÃ©todos de cÃ¡lculo diferencial, para que la red implemente finalmente una funciÃ³n compleja deseada.



  2. Principales arquitecturas: MLP, CNN, RNN.


  Un PerceptrÃ³n Multicapa (MLP, por sus siglas en inglÃ©s) es una red neuronal de tipo feed-forward (alimentaciÃ³n hacia adelante) organizada en capas secuenciales de neuronas. En su forma mÃ¡s general, un MLP consta de tres tipos de capas: una capa de entrada, una o varias capas ocultas intermedias, y una capa de salida. La capa de entrada recibe directamente las seÃ±ales o caracterÃ­sticas del problema: por ejemplo, en clasificaciÃ³n de imÃ¡genes cada pÃ­xel de la imagen puede corresponder a una neurona de entrada. Estas neuronas de entrada simplemente transmiten los valores hacia la siguiente capa, sin realizar aÃºn una transformaciÃ³n significativa. Luego, cada capa oculta toma las salidas de la capa anterior como sus entradas, aplica las operaciones neuronales (suma ponderada y funciÃ³n de activaciÃ³n) en cada neurona, y pasa sus resultados a la siguiente capa. Las capas ocultas, al tener neuronas totalmente conectadas (tambiÃ©n llamadas capas densas) con las neuronas de la capa previa, son las encargadas de ir extrayendo caracterÃ­sticas abstractas y relaciones no obvias en los datos, gracias a las funciones de activaciÃ³n que introducen no linealidad. Una red con mÃ¡s de una capa oculta se considera ya una red profunda, y en teorÃ­a cuantas mÃ¡s capas (y neuronas) se dispongan, mayor es la capacidad de aproximar funciones complejas â€“ aunque tambiÃ©n aumenta la dificultad de entrenar el modelo y el riesgo de sobreajuste.
Finalmente, la capa de salida produce el resultado final de la red. El nÃºmero de neuronas en la capa de salida depende de la tarea: para un problema de clasificaciÃ³n con $K$ clases posibles, tÃ­picamente se usan $K$ neuronas de salida (cada una estimando la pertenencia a una clase) mientras que para un problema de regresiÃ³n suele haber una Ãºnica neurona de salida (que emite un valor continuo). En un clasificador MLP, la capa de salida suele emplear una funciÃ³n de activaciÃ³n apropiada para generar una probabilidad o puntuaciÃ³n por cada clase. Por ejemplo, en problemas de clasificaciÃ³n binaria (sÃ­/no) es comÃºn usar una activaciÃ³n Sigmoide que entrega valores entre 0 y 1, y en problemas multiclase se utiliza la funciÃ³n Softmax, que convierte el vector de activaciones de salida en una distribuciÃ³n de probabilidad (todas las salidas entre 0 y 1 y sumando 1). De esta forma, el Ã­ndice de la neurona de salida con mayor activaciÃ³n indicarÃ¡ la clase predicha por la red. Cada neurona de la capa de salida toma en cuenta todas las activaciones de la Ãºltima capa oculta (por eso es una capa densa), combinÃ¡ndolas segÃºn sus pesos finales para producir la decisiÃ³n.
El flujo de datos en un MLP ocurre Ãºnicamente hacia adelante (no hay conexiones recurrentes en este modelo): las entradas se propagan a travÃ©s de las capas ocultas hasta obtener la salida. Este tipo de arquitectura se denomina red neuronal alimentada hacia adelante (feed-forward neural network). Gracias a la presencia de capas ocultas con activaciones no lineales, los MLP pueden modelar relaciones no lineales complejas en los datos.  Por ejemplo, un MLP con suficientes neuronas puede aproximar funciones continuas arbitrarias en $\mathbb{R}^n$ (segÃºn el teorema de aproximaciÃ³n universal). La capacidad de aprendizaje del MLP radica en que sus pesos sinÃ¡pticos se ajustan durante el entrenamiento para extraer los patrones internos de los datos: inicialmente los pesos se asignan con valores aleatorios, y tras el entrenamiento acaban representando la contribuciÃ³n que cada neurona de capa previa tiene sobre las neuronas de la capa siguiente en la tarea de predicciÃ³n correcta. En resumen, la arquitectura de un MLP es una composiciÃ³n jerÃ¡rquica de unidades de cÃ¡lculo simples (neuronas artificiales) distribuidas en capas, donde cada capa transforma progresivamente las representaciones de los datos, permitiendo a la red resolver tareas de clasificaciÃ³n o regresiÃ³n mÃ¡s allÃ¡ de las capacidades de modelos lineales simples [3].
Funciones de activaciÃ³n en redes neuronales
Las funciones de activaciÃ³n son un componente esencial de las neuronas artificiales, pues introducen no linealidad en el modelo y permiten a la red neuronal aprender patrones complejos. Sin funciones de activaciÃ³n, un MLP con capas ocultas equivaldrÃ­a a una simple combinaciÃ³n lineal y perderÃ­a su potencia expresiva. Existen diversas funciones de activaciÃ³n, cada una con caracterÃ­sticas y usos particulares. A continuaciÃ³n se describen las mÃ¡s comunes, enfatizando su rol en una red multicapa tÃ­pica:
â€¢	Sigmoide (logÃ­stica): Es una funciÃ³n en forma de â€œSâ€ que toma cualquier valor real y lo comprime en el rango $0$ a $1$. Se define como $f(z) = \frac{1}{1 + e^{-z}}$. Fue muy usada histÃ³ricamente tanto en capas ocultas como de salida. En la capa de salida de un clasificador binario, una sigmoide puede interpretarse como la probabilidad estimada de la clase positiva. Sus ventajas incluyen su interpretabilidad probabilÃ­stica y su carÃ¡cter suave; sin embargo, tiene el inconveniente de que para valores $z$ muy grandes o muy pequeÃ±os la derivada se aproxima a cero (regiÃ³n de saturaciÃ³n), lo que puede hacer lento el aprendizaje (gradientes muy pequeÃ±os, fenÃ³meno conocido como desvanecimiento del gradiente).
â€¢	Tanh (Tangente hiperbÃ³lica): Es similar a la sigmoide pero mapea los valores de entrada al rango $-1$ a $1$. Su fÃ³rmula es $f(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$. Al ser antisimÃ©trica (centrada en 0), a menudo convergÃ­a mejor que la sigmoide en redes profundas tradicionales, y fue popular en capas ocultas antes de la apariciÃ³n de ReLU. Aun asÃ­, en magnitudes grandes tambiÃ©n tiende a saturarse con derivadas cercanas a cero, compartiendo el problema de desvanecimiento del gradiente.
â€¢	ReLU (Rectified Linear Unit): Actualmente es una de las funciones de activaciÃ³n mÃ¡s utilizadas en capas ocultas de redes profundas. Es muy sencilla: $f(z) = \max(0, z)$, es decir, produce 0 si $z$ es negativo y produce $z$ sin cambios si es positivo. La ReLU tiene varias ventajas: computacionalmente es barata de calcular, ayuda a mitigar el problema del gradiente desvanecido (pues su derivada es 1 para $z>0$), y tiende a inducir esparsidad en la activaciÃ³n de las neuronas (ya que muchas neuronas pueden quedar en 0 para una dada entrada, reduciendo la interacciÃ³n compleja entre parÃ¡metros). No obstante, puede presentar el problema de "neurona muerta", cuando un peso se ajusta de tal forma que la neurona nunca vuelve a activarse (queda atascada en la regiÃ³n $z<0$ dando siempre salida 0). Variantes de ReLU, como Leaky ReLU o ELU, buscan aliviar este problema introduciendo una pequeÃ±a pendiente no nula para $z$ negativos.
â€¢	Softmax: Es la funciÃ³n de activaciÃ³n estÃ¡ndar utilizada en la capa de salida para problemas de clasificaciÃ³n multiclase (mÃ¡s de dos clases). La funciÃ³n Softmax toma un vector de $K$ valores reales (las activaciones de las $K$ neuronas de salida) y los transforma en un vector de probabilidades de tamaÃ±o $K$ que suman 1. MatemÃ¡ticamente, para cada componente $j$ del vector de salida $z$, $\text{softmax}(z)j = \frac{\exp(z_j)}{\sum{k=1}^{K} \exp(z_k)}$. Esto â€œresaltaâ€ el mayor valor y suprime los mÃ¡s bajos, generando una distribuciÃ³n donde tÃ­picamente una clase obtiene la mayor probabilidad y las demÃ¡s quedan con valores pequeÃ±os. Gracias a Softmax, un MLP puede asignar de forma natural una probabilidad a cada clase posible, facilitando la interpretaciÃ³n de la salida y permitiendo entrenar la red usando como pÃ©rdida la entropÃ­a cruzada categÃ³rica (la cual compara la distribuciÃ³n predicha con la distribuciÃ³n objetivo, que suele ser una one-hot vector que indica la clase correcta). En la prÃ¡ctica, Softmax se usa junto con entropÃ­a cruzada porque esta combinaciÃ³n tiene propiedades matemÃ¡ticas que aceleran y estabilizan el entrenamiento de clasificadores multiclase.
En resumen, la elecciÃ³n de la funciÃ³n de activaciÃ³n depende del rol de la neurona y la naturaleza del problema. ReLU suele preferirse en las capas ocultas por su eficiencia y buen desempeÃ±o en redes profundas, evitando saturaciÃ³n de gradientes. Para la capa de salida, sigmoide funciona bien en salidas binarias, mientras que Softmax es la elecciÃ³n obligada para salidas multiclase mutuamente excluyentes. Comprender las caracterÃ­sticas de cada funciÃ³n (rango de salida, derivadas, comportamiento para distintos $z$) es crucial al diseÃ±ar e implementar una red neuronal, pues influye directamente en la capacidad de aprendizaje y la velocidad de convergencia del modelo [3].

  
  
  3. Algoritmos de entrenamiento: backpropagation, optimizadores.

Entrenar una red neuronal implica ajustar sus pesos iterativamente para que las predicciones de la red se acerquen lo mÃ¡s posible a las salidas deseadas para las muestras de entrenamiento. El procedimiento estÃ¡ndar de entrenamiento para un MLP es el aprendizaje supervisado mediante el algoritmo de retropropagaciÃ³n combinado con un mÃ©todo de optimizaciÃ³n por descenso de gradiente. En tÃ©rminos generales, el proceso consta de los siguientes pasos en cada iteraciÃ³n (epoch):
1.	PropagaciÃ³n hacia adelante: Se introduce una muestra (o un lote de muestras) de entrenamiento en la red y se calcula la salida estimada pasando por las capas hasta la salida. Con esa salida y la etiqueta esperada, se calcula el error o pÃ©rdida (por ejemplo, usando la funciÃ³n de pÃ©rdida definida, como entropÃ­a cruzada o MSE).
2.	CÃ¡lculo de gradientes (backpropagation): A continuaciÃ³n, se computan las derivadas parciales de la pÃ©rdida con respecto a cada peso de la red, utilizando la retropropagaciÃ³n del error a travÃ©s de las capas. Como se explicÃ³ en secciones anteriores, esto se logra aplicando la regla de la cadena desde la salida hacia la entrada, distribuyendo el error hacia cada conexiÃ³n segÃºn su contribuciÃ³n. El resultado es un gradiente para cada peso $w$ (y bias) que indica si aumentar o disminuir ese peso reducirÃ¡ el error, y en quÃ© magnitud.
3.	ActualizaciÃ³n de pesos: Finalmente, se ajustan los pesos en la direcciÃ³n opuesta al gradiente (de ahÃ­ descenso de gradiente), pues se busca minimizar la pÃ©rdida. Un modelo sencillo de actualizaciÃ³n es: $w := w - \eta \frac{\partial L}{\partial w}$, donde $\eta$ es la tasa de aprendizaje (un factor de paso predeterminado) y $\frac{\partial L}{\partial w}$ es el gradiente del peso $w$. Este paso se repite para todos los pesos de la red. DespuÃ©s, se toma la siguiente muestra o lote de muestras y se repite el ciclo muchas veces.
El mÃ©todo clÃ¡sico descrito es el Descenso de Gradiente en batch completo, que utiliza todo el conjunto de entrenamiento para calcular el gradiente en cada iteraciÃ³n. Sin embargo, en la prÃ¡ctica esto suele ser ineficiente para grandes conjuntos de datos. Por ello es mÃ¡s comÃºn usar Descenso de Gradiente EstocÃ¡stico (SGD) o por mini-lotes. En SGD, los pesos se actualizan con cada ejemplo de entrenamiento individual, lo cual introduce cierta aleatoriedad (ruido) en las actualizaciones pero puede ayudar a escapar de Ã³ptimos locales poco profundos. En el enfoque de mini-lotes, se calcula el gradiente en pequeÃ±os lotes (por ejemplo, 32 o 64 muestras) en cada iteraciÃ³n, consiguiendo un equilibrio entre estabilidad y velocidad. En cualquier caso, SGD y sus variantes siguen la misma idea fundamental: moverse gradualmente en el espacio de parÃ¡metros en la direcciÃ³n que reduce el error.
A lo largo de los aÃ±os se han desarrollado numerosos optimizadores avanzados que modifican la forma en que se actualizan los pesos para lograr convergencia mÃ¡s rÃ¡pida y estable. Uno de los mÃ¡s utilizados actualmente es Adam (por Adaptive Moment Estimation), propuesto por Kingma y Ba en 2015 [4]. El algoritmo Adam combina lo mejor de dos tÃ©cnicas previas, AdaGrad y RMSProp, para adaptar dinÃ¡micamente la tasa de aprendizaje de cada peso. En esencia, Adam acumula de forma exponencialmente decreciente un promedio de los gradientes pasados (esto actÃºa como un momentum, suavizando las oscilaciones) y un promedio de los cuadrados de los gradientes (para normalizar la magnitud de las actualizaciones). Gracias a esto, cada peso tiene su paso de aprendizaje ajustado individualmente: si un peso ha tenido gradientes grandes recientemente, se le asignarÃ¡ un paso efectivo mÃ¡s pequeÃ±o; por el contrario, si el gradiente ha sido pequeÃ±o, se le permite un paso relativamente mayor. Esta adaptaciÃ³n individual hace a Adam muy eficiente en problemas con datos ruidosos o gradientes escasos, proporcionando una convergencia rÃ¡pida y a menudo robusta. AdemÃ¡s, Adam es menos sensible a la elecciÃ³n manual de la tasa de aprendizaje inicial comparado con SGD puro, lo que facilita su uso sin mucha calibraciÃ³n de hiperparÃ¡metros. Por estas razones, Adam se ha convertido en el optimizador por defecto en muchas aplicaciones de deep learning.
Otros optimizadores notables incluyen Momentum SGD (que acumula un porcentaje del gradiente anterior para acelerar direcciones persistentes), RMSProp (que ajusta la tasa de aprendizaje basÃ¡ndose en el promedio mÃ³vil de magnitudes recientes de gradiente) y AdaGrad (que disminuye progresivamente la tasa de aprendizaje para cada peso en proporciÃ³n a la suma de los cuadrados de sus gradientes, Ãºtil para manejar caracterÃ­sticas escasas). Cada optimizador tiene sus ventajas y escenarios ideales, pero en general todos buscan mejorar la rapidez de aprendizaje y la capacidad de escapar de mÃ­nimos locales profundos en el paisaje de error. En la implementaciÃ³n prÃ¡ctica de estos algoritmos, es importante tambiÃ©n aplicar tÃ©cnicas como regularizaciÃ³n (por ejemplo, dropout, regularizaciÃ³n $L^2$) y ajuste de hiperparÃ¡metros (learning rate, tamaÃ±o de mini-lote, etc.) para asegurar que el modelo generalice bien y converja de manera adecuada [3][5].
Consideraciones prÃ¡cticas para la implementaciÃ³n en C++
La implementaciÃ³n de un perceptrÃ³n multicapa en un lenguaje de bajo nivel como C++ conlleva una serie de desafÃ­os y decisiones de diseÃ±o orientadas a maximizar la eficiencia y garantizar la correcta gestiÃ³n de recursos. A diferencia de entornos de alto nivel (Python con bibliotecas como TensorFlow o PyTorch), en C++ el programador tiene control explÃ­cito sobre los detalles de memoria y puede optimizar el cÃ³digo a bajo nivel, pero tambiÃ©n debe hacerse cargo de tareas que en otros entornos son automÃ¡ticas (como la liberaciÃ³n de memoria o la derivaciÃ³n simbÃ³lica). A continuaciÃ³n, se destacan tres consideraciones prÃ¡cticas clave al implementar un MLP en C++:
â€¢	Eficiencia y cÃ³mputo numÃ©rico: El entrenamiento de redes neuronales involucra multitud de operaciones algebraicas (multiplicaciones de matrices, sumas vectoriales, etc.) que pueden ser costosas computacionalmente. En C++, es crucial aprovechar estructuras de datos y algoritmos eficientes para estas operaciones. Por ejemplo, suele implementarse una clase Matrix optimizada para representar matrices y vectores, con sobrecarga de operadores para multiplicaciÃ³n de matrices, y con rutinas internas que aprovechen tÃ©cnicas de vectorizaciÃ³n (SIMD) o incluso paralelismo multi-hilo para grandes multiplicaciones. En el proyecto descrito, se desarrollÃ³ Matrix.h/cpp especÃ­ficamente para operaciones matriciales eficientes, ya que las capas densas del MLP realizan bÃ¡sicamente multiplicaciones de matrices entre los datos de entrada y los pesos. Un enfoque comÃºn es utilizar bibliotecas de Ã¡lgebra lineal optimizadas (por ejemplo Eigen, BLAS, etc.), aunque tambiÃ©n es posible escribir las rutinas a medida. AdemÃ¡s, se puede recurrir a estrategias como mini-batch para procesar varias muestras juntas y aprovechar mejor la localidad de datos en cache. En pruebas de rendimiento, una implementaciÃ³n C++ bien optimizada puede multiplicar matrices grandes (ej. 1000x1000) en pocos segundos, lo cual es esencial para que el entrenamiento de la red sea razonablemente rÃ¡pido. Asimismo, es importante compilar el cÃ³digo en modo optimizado (por ejemplo, usando banderas de optimizaciÃ³n de O2/O3 en compiler) y, de ser posible, explotar caracterÃ­sticas de hardware especÃ­ficas (instrucciones SIMD, subprocesamiento, etc.). En resumen, la eficiencia computacional en C++ proviene tanto de elegir buenos algoritmos como de aprovechar al mÃ¡ximo los recursos de hardware disponibles.
â€¢	InicializaciÃ³n de pesos: Una consideraciÃ³n crÃ­tica al empezar el entrenamiento de una red es cÃ³mo se inicializan los pesos sinÃ¡pticos. En C++, tras reservar la memoria para los arreglos/matrices de pesos, es necesario asignarles valores iniciales. Una mala inicializaciÃ³n (por ejemplo, todos ceros) impedirÃ­a el aprendizaje al provocar simetrÃ­as que la retropropagaciÃ³n no puede romper. Lo habitual es inicializar los pesos con pequeÃ±os valores aleatorios. En el proyecto de ejemplo, seguramente se utiliza una rutina para randomizar las matrices de pesos con valores aleatorios en un rango pequeÃ±o (por ejemplo, entre -0.1 y 0.1). Esto se aprecia en la clase Matrix que ofrece el mÃ©todo randomize(min, max) para llenar la matriz con valores aleatorios uniformes en [min, max]. Adicionalmente, existen esquemas de inicializaciÃ³n mÃ¡s sofisticados que se recomiendan para redes profundas: Xavier/Glorot (inicializaciÃ³n que considera el tamaÃ±o de la capa para mantener la varianza de activaciones) o He (especialmente Ãºtil con ReLU, asignando varianza proporcional al nÃºmero de entradas de la neurona). Implementar estas inicializaciones en C++ implica calcular el intervalo adecuado de aleatoriedad en funciÃ³n del nÃºmero de neuronas de entrada/salida de cada capa. Una buena inicializaciÃ³n acelera la convergencia y evita problemas como la saturaciÃ³n inicial de neuronas. Por otro lado, los sesgos (bias) a menudo se inicializan en cero o pequeÃ±os valores constantes, ya que aÃ±adir un bias igual en todas las neuronas no rompe la simetrÃ­a como sÃ­ ocurrirÃ­a con los pesos. En suma, en la implementaciÃ³n C++ se debe prestar atenciÃ³n a proveer funciones de inicializaciÃ³n aleatoria de pesos que sigan buenas prÃ¡cticas de la literatura de redes neuronales.
â€¢	Manejo de memoria y recursos: C++ otorga control manual sobre la memoria, lo que obliga a ser disciplinado para evitar fugas (memory leaks) y garantizar la liberaciÃ³n apropiada de recursos. En el contexto de un MLP, se manejarÃ¡n potencialmente grandes bloques de memoria para almacenar pesos (matrices de dimensiones [n_entradas Ã— n_neuronas]) y datos de entrenamiento. Una estrategia Ãºtil es aprovechar las funcionalidades modernas de C++ como punteros inteligentes (std::unique_ptr, std::shared_ptr) y contenedores estÃ¡ndar (std::vector) para delegar la administraciÃ³n de memoria y asegurar liberaciÃ³n automÃ¡tica al salir de Ã¡mbito. En el proyecto de referencia, por ejemplo, las capas de la red (Layer) se almacenan en un std::vector<std::unique_ptr<Layer>>, de modo que al destruir la red neuronal se destruyen automÃ¡ticamente todas las capas alojadas. Este uso de RAII (InicializaciÃ³n y liberaciÃ³n de recursos garantizada) simplifica el manejo de memoria y evita fugas al no tener que invocar delete manualmente para cada objeto dinÃ¡mico. Otra consideraciÃ³n es reducir en lo posible las copias innecesarias de datos: por ejemplo, pasar referencias o punteros a matrices en las funciones de forward y backward en vez de copiarlas, reutilizar buffers ya reservados para deltas de gradiente, etc. TambiÃ©n es importante alinear correctamente la memoria si se emplean instrucciones vectoriales, y ser consciente del consumo: datasets grandes como MNIST cargados completamente ocupan memoria, por lo que se podrÃ­a optar por lecturas por lotes desde disco si la RAM es limitada. Por Ãºltimo, la depuraciÃ³n de una red neuronal en C++ puede ser complicada; es recomendable diseÃ±ar desde el inicio pruebas unitarias para componentes (como multiplicaciÃ³n de matrices, forward/backward de una capa, etc.) â€“ tal como se hizo en el proyecto con una baterÃ­a de tests unitarios â€“ que permitan verificar que cada pieza funciona correctamente antes de integrar todo. Esto ayuda a detectar a tiempo errores de implementaciÃ³n que podrÃ­an llevar a que el entrenamiento no converja. En definitiva, una implementaciÃ³n en C++ de un MLP bien diseÃ±ada debe equilibrar el aprovechamiento mÃ¡ximo del hardware con una gestiÃ³n cuidadosa de la memoria, utilizando las herramientas del lenguaje para mantener un cÃ³digo seguro y eficiente.


### 2. DiseÃ±o e implementaciÃ³n

#### 2.1 Arquitectura de la soluciÃ³n

* **Patrones de diseÃ±o**:

* Factory Pattern: Para la creaciÃ³n de diferentes tipos de capas y optimizadores, permitiendo extensibilidad del sistema.

// LayerFactory.h

class LayerFactory {

public:
    
    static std::unique_ptr<Layer> createLayer(LayerType type, int inputSize, int outputSize);
    
    static std::unique_ptr<ActivationFunction> createActivation(ActivationType type);

};

* **Strategy Pattern**: Para algoritmos de optimizaciÃ³n intercambiables (SGD, Adam, RMSprop).

// OptimizerStrategy.h

class OptimizerStrategy {

public:
    
    virtual void updateWeights(Matrix& weights, const Matrix& gradients) = 0;
    
    virtual ~OptimizerStrategy() = default;

};

**Observer Pattern**: Para monitoreo del progreso de entrenamiento.

// TrainingObserver.h

class TrainingObserver {

public:
    
    virtual void onEpochComplete(int epoch, double loss, double accuracy) = 0;
    
    virtual void onTrainingComplete() = 0;

};

**ESTRUCTURA DE CARPETAS IMPLEMENTADAS**:

```text
proyecto-final/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ Matrix.h/cpp            # Operaciones matriciales optimizadas
â”‚   â”‚   â”œâ”€â”€ NeuralNetwork.h/cpp     # Clase principal del modelo
â”‚   â”‚   â”œâ”€â”€ Dataset.h/cpp           # Cargador de datos MNIST
â”‚   â”‚   â””â”€â”€ Utils.h/cpp             # Funciones auxiliares
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ Layer.h                 # Interfaz base para capas
â”‚   â”‚   â”œâ”€â”€ DenseLayer.h/cpp        # Capa totalmente conectada
â”‚   â”‚   â”œâ”€â”€ ActivationLayer.h/cpp   # Capas de activaciÃ³n
â”‚   â”‚   â””â”€â”€ LayerFactory.h/cpp      # Factory para creaciÃ³n de capas
â”‚   â”œâ”€â”€ optimizers/
â”‚   â”‚   â”œâ”€â”€ Optimizer.h             # Interfaz base para optimizadores
â”‚   â”‚   â”œâ”€â”€ SGD.h/cpp               # Gradiente descendente estocÃ¡stico
â”‚   â”‚   â”œâ”€â”€ Adam.h/cpp              # Optimizador Adam
â”‚   â”‚   â””â”€â”€ RMSprop.h/cpp           # Optimizador RMSprop
â”‚   â”œâ”€â”€ activations/
â”‚   â”‚   â”œâ”€â”€ ReLU.h/cpp              # FunciÃ³n de activaciÃ³n ReLU
â”‚   â”‚   â”œâ”€â”€ Sigmoid.h/cpp           # FunciÃ³n de activaciÃ³n Sigmoid
â”‚   â”‚   â””â”€â”€ Softmax.h/cpp           # FunciÃ³n de activaciÃ³n Softmax
â”‚   â”œâ”€â”€ losses/
â”‚   â”‚   â”œâ”€â”€ CrossEntropy.h/cpp      # EntropÃ­a cruzada categÃ³rica
â”‚   â”‚   â””â”€â”€ MeanSquaredError.h/cpp  # Error cuadrÃ¡tico medio
â”‚   â””â”€â”€ main.cpp                    # Programa principal
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_matrix.cpp             # Pruebas de operaciones matriciales
â”‚   â”œâ”€â”€ test_layers.cpp             # Pruebas de capas individuales
â”‚   â”œâ”€â”€ test_optimizers.cpp         # Pruebas de optimizadores
â”‚   â””â”€â”€ test_integration.cpp        # Pruebas de integraciÃ³n completa
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ mnist/                      # Dataset MNIST
â”‚   â””â”€â”€ examples/                   # Datos de ejemplo
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md             # DocumentaciÃ³n tÃ©cnica
â”‚   â””â”€â”€ demo.mp4                    # Video demostrativo
â””â”€â”€ CMakeLists.txt                  # ConfiguraciÃ³n de compilaciÃ³n
```

#### Componentes principales implementados:

### Clase Neuronal Network:

NÃºcleo del modelo que coordina todas las operaciones.

```cpp
private:
    std::vector<std::unique_ptr<Layer>> layers;
    std::unique_ptr<LossFunction> lossFunction;
    std::unique_ptr<OptimizerStrategy> optimizer;
    std::vector<double> trainingLoss;

public:
    void addLayer(std::unique_ptr<Layer> layer);

    Matrix forward(const Matrix& input);
    void backward(const Matrix& predicted, const Matrix& actual);
    void train(const std::vector<Matrix>& trainX, const std::vector<Matrix>& trainY,
               int epochs, int batchSize = 32);
    double evaluate(const std::vector<Matrix>& testX, const std::vector<Matrix>& testY);
```

### Clase Dense Layer: 


```cpp
class DenseLayer : public Layer {
private:
    Matrix weights;
    Matrix biases;
    Matrix lastInput;

public:
    DenseLayer(int inputSize, int outputSize);
    Matrix forward(const Matrix& input) override;
    Matrix backward(const Matrix& gradOutput) override;
    void updateWeights(OptimizerStrategy* optimizer) override;
};
```

### Optimizador Adam:

ImplementaciÃ³n del algoritmo o de optimizaciÃ³n Adam

```cpp
class Adam : public OptimizerStrategy {
private:
    double learningRate, beta1, beta2, epsilon;
    std::unordered_map<void*, Matrix> firstMoments, secondMoments;

public:
    Adam(double lr = 0.001, double b1 = 0.9, double b2 = 0.999);
    void updateWeights(Matrix& weights, const Matrix& gradients) override;
};
```

#### 2.2 Manual de uso y casos de prueba

* **CÃ³mo ejecutar**: `./build/neural_net_demo input.csv output.csv`

```bash
# Compilar el proyecto
mkdir build && cd build
cmake -DCMAKE_BUILD_TYPE=Release ..

make -j$(nproc)

# Ejecutar entrenamiento bÃ¡sico
./neural_net_demo --train data/mnist/train.csv --test data/mnist/test.csv --epochs 50

# Ejecutar con configuraciÃ³n personalizada
./neural_net_demo --config config/network.json --output results/

# Modo evaluaciÃ³n solamente
./neural_net_demo --evaluate --model saved_models/best_model.bin --test data/mnist/test.csv
```

* **Ejemplo de uso pragmÃ¡tico**:

```cpp
#include "NeuralNetwork.h"
#include "DenseLayer.h"
#include "ActivationLayer.h"
#include "Adam.h"
#include "CrossEntropy.h"

int main() {
    // Crear la red neuronal
    NeuralNetwork network;

    // Arquitectura: 784 -> 128 -> 64 -> 10
    network.addLayer(std::make_unique<DenseLayer>(784, 128));
    network.addLayer(std::make_unique<ActivationLayer>(std::make_unique<ReLU>()));
    network.addLayer(std::make_unique<DenseLayer>(128, 64));
    network.addLayer(std::make_unique<ActivationLayer>(std::make_unique<ReLU>()));
    network.addLayer(std::make_unique<DenseLayer>(64, 10));
    network.addLayer(std::make_unique<ActivationLayer>(std::make_unique<Softmax>()));

    // Configurar optimizaciÃ³n
    network.setOptimizer(std::make_unique<Adam>(0.001));
    network.setLossFunction(std::make_unique<CrossEntropy>());

    // Cargar datos
    DataLoader loader;
    auto [trainX, trainY] = loader.loadMNIST("data/mnist_train.csv");
    auto [testX, testY] = loader.loadMNIST("data/mnist_test.csv");

    // Entrenar
    network.train(trainX, trainY, 50, 64);

    // Evaluar
    double accuracy = network.evaluate(testX, testY);
    std::cout << "PrecisiÃ³n: " << accuracy * 100 << "%" << std::endl;

    return 0;
}
```

* **Casos de prueba**:

  * Test unitario de capa densa.
 

```cpp
TEST(DenseLayerTest, ForwardPass) {
    DenseLayer layer(3, 2);
    Matrix input(3, 1);
    input(0,0) = 1.0; input(1,0) = 2.0; input(2,0) = 3.0;

    Matrix output = layer.forward(input);

    EXPECT_EQ(output.getRows(), 2);
    EXPECT_EQ(output.getCols(), 1);
    // Verificar que la salida tiene dimensiones correctas
}

TEST(DenseLayerTest, BackwardPass) {
    DenseLayer layer(2, 1);
    Matrix input(2, 1);
    input(0,0) = 1.0; input(1,0) = 2.0;

    Matrix output = layer.forward(input);

    Matrix gradOutput(1, 1);
    gradOutput(0,0) = 1.0;

    Matrix gradInput = layer.backward(gradOutput);

    EXPECT_EQ(gradInput.getRows(), 2);
    EXPECT_EQ(gradInput.getCols(), 1);
}
```

  * Test de funciÃ³n de activaciÃ³n ReLU.

```cpp
TEST(ReLUTest, ForwardPass) {
    ReLU relu;
    Matrix input(2, 2);
    input(0,0) = -1.0; input(0,1) = 2.0;
    input(1,0) = 0.0;  input(1,1) = -3.0;

    Matrix output = relu.forward(input);

    EXPECT_EQ(output(0,0), 0.0);  // -1 -> 0
    EXPECT_EQ(output(0,1), 2.0);  //  2 -> 2
    EXPECT_EQ(output(1,0), 0.0);  //  0 -> 0
    EXPECT_EQ(output(1,1), 0.0);  // -3 -> 0
}
```

```cpp
TEST(ReLUTest, BackwardPass) {
    ReLU relu;
    Matrix input(2, 1);
    input(0,0) = 1.0; input(1,0) = -1.0;

    Matrix gradOutput(2, 1);
    gradOutput(0,0) = 1.0; gradOutput(1,0) = 1.0;

    Matrix gradInput = relu.backward(gradOutput, input);

    EXPECT_EQ(gradInput(0,0), 1.0);  // input > 0: gradiente pasa
    EXPECT_EQ(gradInput(1,0), 0.0);  // input < 0: gradiente = 0
}
```
  
  * Test de convergencia en dataset de ejemplo.

```cpp
TEST(IntegrationTest, XORProblem) {
    // Crear red para problema XOR
    NeuralNetwork network;
    network.addLayer(std::make_unique<DenseLayer>(2, 4));
    network.addLayer(std::make_unique<ActivationLayer>(std::make_unique<ReLU>()));
    network.addLayer(std::make_unique<DenseLayer>(4, 1));
    network.addLayer(std::make_unique<ActivationLayer>(std::make_unique<Sigmoid>()));

    network.setOptimizer(std::make_unique<Adam>(0.01));
    network.setLossFunction(std::make_unique<MeanSquaredError>());

    // Datos XOR
    std::vector<Matrix> inputs = {
        Matrix({{0, 0}}), Matrix({{0, 1}}),
        Matrix({{1, 0}}), Matrix({{1, 1}})
    };

    std::vector<Matrix> targets = {
        Matrix({{0}}), Matrix({{1}}),
        Matrix({{1}}), Matrix({{0}})
    };

    // Entrenar
    network.train(inputs, targets, 1000, 4);

    // Verificar convergencia
    double accuracy = network.evaluate(inputs, targets);
    EXPECT_GT(accuracy, 0.9);  // Al menos 90% de precisiÃ³n
}
```

 * Test de rendimiento:

```cpp
TEST(PerformanceTest, LargeMatrixMultiplication) {
    auto start = std::chrono::high_resolution_clock::now();

    Matrix a(1000, 1000);
    Matrix b(1000, 1000);
    a.randomize(-1.0, 1.0);
    b.randomize(-1.0, 1.0);

    Matrix c = a * b;

    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);

    std::cout << "Tiempo multiplicaciÃ³n 1000x1000: " 
              << duration.count() << " ms" << std::endl;

    EXPECT_LT(duration.count(), 3000);  // Menos de 3 segundos
}
```
 * CONFIGURACIÃ“N AVANZADA:

   El sistema soporta configuraciÃ³n mediante archivos JSON:


```json
{
  "network": {
    "layers": [
      { "type": "dense", "input_size": 784, "output_size": 256 },
      { "type": "activation", "function": "relu" },
      { "type": "dense", "input_size": 256, "output_size": 128 },
      { "type": "activation", "function": "relu" },
      { "type": "dense", "input_size": 128, "output_size": 10 },
      { "type": "activation", "function": "softmax" }
    ]
  },
  "training": {
    "optimizer": "adam",
    "learning_rate": 0.001,
    "batch_size": 64,
    "epochs": 100
  },
  "evaluation": {
    "validation_split": 0.2,
    "metrics": ["accuracy", "loss", "f1_score"]
  }
}
```

## ðŸ› ï¸ Optimizaciones implementadas:

1. **MultiplicaciÃ³n de matrices cache-friendly**: Reordenamiento de bucles para mejor localidad de memoria  
2. **ParalelizaciÃ³n con OpenMP**: Operaciones matriciales paralelizadas  
3. **Memory pooling**: ReutilizaciÃ³n de matrices temporales  
4. **Batch processing**: Procesamiento eficiente de lotes  
5. **InicializaciÃ³n Xavier**: InicializaciÃ³n Ã³ptima de pesos  
6. **Gradient clipping**: PrevenciÃ³n de explosiÃ³n de gradientes  

---

## ðŸ“Š MÃ©tricas de rendimiento logradas:

| **MÃ©trica**                       | **Valor**                                 |
|----------------------------------|-------------------------------------------|
| PrecisiÃ³n en MNIST               | 94.2%                                     |
| Tiempo de entrenamiento          | 45 minutos (50 Ã©pocas)                    |
| OptimizaciÃ³n en memoria          | 35% vs sin pipeline en entrenamiento bÃ¡sico |
| Speedup con OpenMP               | 2.3Ã— en matrices grandes                  |
| Estabilidad numÃ©rica             | Sin overflow/underdflow en 1000+ ejecuciones |





> *Personalizar rutas, comandos y casos reales.*

---

### 3. EjecuciÃ³n

> **Demo de ejemplo**: Video/demo alojado en `docs/demo.mp4`.
> Pasos:
>
> 1. Preparar datos de entrenamiento (formato CSV).
> 2. Ejecutar comando de entrenamiento.
> 3. Evaluar resultados con script de validaciÃ³n.

---

### 4. AnÃ¡lisis del rendimiento

* **MÃ©tricas de ejemplo**:

  * Iteraciones: 1000 Ã©pocas.
  * Tiempo total de entrenamiento: 2m30s.
  * PrecisiÃ³n final: 92.5%.
* **Ventajas/Desventajas**:

  * * CÃ³digo ligero y dependencias mÃ­nimas.
  * â€“ Sin paralelizaciÃ³n, rendimiento limitado.
* **Mejoras futuras**:

  * Uso de BLAS para multiplicaciones (JustificaciÃ³n).
  * Paralelizar entrenamiento por lotes (JustificaciÃ³n).

---

### 5. Trabajo en equipo

| Tarea                     | Miembro  | Rol                       |
| ------------------------- | -------- | ------------------------- |
| InvestigaciÃ³n teÃ³rica     | Alumno A | Documentar bases teÃ³ricas |
| DiseÃ±o de la arquitectura | Alumno B | UML y esquemas de clases  |
| ImplementaciÃ³n del modelo | Alumno C | CÃ³digo C++ de la NN       |
| Pruebas y benchmarking    | Alumno D | GeneraciÃ³n de mÃ©tricas    |
| DocumentaciÃ³n y demo      | Alumno E | Tutorial y video demo     |

> *Actualizar con tareas y nombres reales.*

---

### 6. Conclusiones

* **Logros**: Implementar NN desde cero, validar en dataset de ejemplo.
* **EvaluaciÃ³n**: Calidad y rendimiento adecuados para propÃ³sito acadÃ©mico.
* **Aprendizajes**: ProfundizaciÃ³n en backpropagation y optimizaciÃ³n.
* **Recomendaciones**: Escalar a datasets mÃ¡s grandes y optimizar memoria.

---

### 7. BibliografÃ­a

> *Actualizar con bibliografia utilizada, al menos 4 referencias bibliograficas y usando formato IEEE de referencias bibliograficas.*

---

### Licencia

Este proyecto usa la licencia **MIT**. Ver [LICENSE](LICENSE) para detalles.

---
