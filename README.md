![image](https://github.com/user-attachments/assets/83c618fb-e39d-44db-9b9a-b9d6509d6ef4)[![Review Assignment Due Date](https://classroom.github.com/assets/deadline-readme-button-22041afd0340ce965d47ae6ef1cefeee28c7c493a6346c4f15d667ab976d596c.svg)](https://classroom.github.com/a/Lj3YlzJp)
# Proyecto Final 2025-1: AI Neural Network
## **CS2013 ProgramaciÃ³n III** Â· Informe Final

### **DescripciÃ³n**

> Ejemplo: ImplementaciÃ³n de una red neuronal multicapa en C++ para clasificaciÃ³n de dÃ­gitos manuscritos.

### Contenidos

1. [Datos generales](#datos-generales)
2. [Requisitos e instalaciÃ³n](#requisitos-e-instalaciÃ³n)
3. [InvestigaciÃ³n teÃ³rica](#1-investigaciÃ³n-teÃ³rica)
4. [DiseÃ±o e implementaciÃ³n](#2-diseÃ±o-e-implementaciÃ³n)
5. [EjecuciÃ³n](#3-ejecuciÃ³n)
6. [AnÃ¡lisis del rendimiento](#4-anÃ¡lisis-del-rendimiento)
7. [Trabajo en equipo](#5-trabajo-en-equipo)
8. [Conclusiones](#6-conclusiones)
9. [BibliografÃ­a](#7-bibliografÃ­a)
10. [Licencia](#licencia)
---

### Datos generales

* **Tema**: Redes Neuronales en AI
* **Grupo**: `group_3_custom_name`
* **Integrantes**:

  * Henrry Andre Valle Enriquez â€“ 202310310 (Responsable de investigaciÃ³n teÃ³rica)
  * JosÃ© Mariano Llacta GonzÃ¡lez â€“ 202410365 (Desarrollo de la arquitectura)
  * Eliseo David Velasquez Diaz â€“ 202410184 (ImplementaciÃ³n del modelo)
  * Alejandro Vargas Rios â€“ 202410089 (Pruebas y benchmarking)
  * Alumno E â€“ 209900005 (DocumentaciÃ³n y demo)

> *Nota: Reemplazar nombres y roles reales.*

---

### Requisitos e instalaciÃ³n

1. **Compilador**: GCC 11 o superior
2. **Dependencias**:

   * CMake 3.18+
   * Eigen 3.4
   * \[Otra librerÃ­a opcional]
3. **InstalaciÃ³n**:

   ```bash
   git clone https://github.com/EJEMPLO/proyecto-final.git
   cd proyecto-final
   mkdir build && cd build
   cmake ..
   make
   ```

> *Ejemplo de repositorio y comandos, ajustar segÃºn proyecto.*

---

### 1. InvestigaciÃ³n teÃ³rica

* **Objetivo**: Explorar fundamentos y arquitecturas de redes neuronales.
* **Contenido de ejemplo**:

  1. Historia y evoluciÃ³n de las NNs.   

El desarrollo de las redes neuronales artificiales se remonta a mediados del siglo XX. En 1943, McCulloch y Pitts propusieron el primer modelo de neurona artificial (una funciÃ³n lÃ³gica umbral), sentando las bases del conexionismo. AÃ±os mÃ¡s tarde, el psicÃ³logo Donald Hebb formulÃ³ en 1949 la regla de aprendizaje que lleva su nombre, enfatizando que la fortaleza de las conexiones neuronales aumenta si ambas neuronas se activan simultÃ¡neamente. Un hito fundamental ocurriÃ³ en 1958, cuando Frank Rosenblatt creÃ³ el PerceptrÃ³n, considerado la primera neurona artificial entrenable.El perceptrÃ³n de Rosenblatt podÃ­a aprender a clasificar patrones simples ajustando pesos sinÃ¡pticos, lo que marcÃ³ el inicio del campo de aprendizaje automÃ¡tico con redes neuronales [1]. Sin embargo, a finales de la dÃ©cada de 1960, las expectativas sobre las redes neuronales sufrieron un revÃ©s. En 1969, Marvin Minsky y Seymour Papert publicaron una crÃ­tica que demostraba limitaciones del perceptrÃ³n de capa simple (por ejemplo, su incapacidad para resolver la funciÃ³n XOR), ademÃ¡s de seÃ±alar la insuficiencia del hardware de la Ã©poca para entrenar redes mÃ¡s complejas. Estas observaciones llevaron a un estancamiento en la investigaciÃ³n de redes neuronales durante varios aÃ±os, periodo a menudo denominado el â€œinvierno de la IAâ€.
El resurgimiento llegÃ³ en la dÃ©cada de 1980 gracias a la introducciÃ³n de redes neuronales de mÃºltiples capas y algoritmos de entrenamiento mÃ¡s eficientes. Un avance clave fue el algoritmo de retropropagaciÃ³n del error (backpropagation), inicialmente descrito por Paul Werbos en 1975 y popularizado en 1986 por Rumelhart, Hinton y Williams [2]. La retropropagaciÃ³n permitiÃ³ ajustar los pesos de redes con una o mÃ¡s capas ocultas propagando hacia atrÃ¡s el error de salida, haciendo factible entrenar los llamados perceptrones multicapa (MLP) de forma supervisada. A partir de entonces, se lograron Ã©xitos en tareas de reconocimiento de patrones que antes eran intratables para redes de una sola capa. Durante los aÃ±os 1990, otros mÃ©todos de aprendizaje automÃ¡tico como las mÃ¡quinas de soporte vectorial cobraron protagonismo, pero las redes neuronales mantuvieron su desarrollo en dominios especÃ­ficos. Ya en el siglo XXI, la combinaciÃ³n de algoritmos mejorados, grandes volÃºmenes de datos y un aumento notable en el poder de cÃ³mputo (especialmente con el uso de GPUs) propiciÃ³ el auge del aprendizaje profundo (deep learning). Modelos con muchas capas ocultas (redes neuronales profundas) comenzaron a superar el estado del arte en reconocimiento de imÃ¡genes, voz y texto alrededor de 2012, dando lugar a la era actual de la IA basada en redes neuronales [5]. En resumen, las redes neuronales han evolucionado desde perceptrones simples hasta arquitecturas profundas complejas, pasando por la etapa 
crucial de los MLP, que sentaron las bases conceptuales de muchos avances modernos.

### Fundamentos matemÃ¡ticos bÃ¡sicos

Las redes neuronales artificiales se inspiran en las neuronas biolÃ³gicas, pero se definen mediante modelos matemÃ¡ticos. La neurona artificial bÃ¡sica recibe una serie de entradas numÃ©ricas $x_1, x_2, \dots, x_n$, cada una asociada a un peso sinÃ¡ptico $w_1, w_2, \dots, w_n$ que representa la importancia de esa entrada. La neurona calcula primero una combinaciÃ³n lineal de sus entradas â€“ comÃºnmente denominada suma ponderada â€“ a la que se le agrega un tÃ©rmino llamado bias o sesgo ($b$). En tÃ©rminos matemÃ¡ticos, el potencial de activaciÃ³n de la neurona (a menudo denotado $z$) es:
z=w1x1+w2x2+â‹¯+wnxn+b.z = w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b.z=w1x1+w2x2+â‹¯+wnxn+b.
Este valor $z$ es entonces transformado por medio de una funciÃ³n de activaciÃ³n no lineal para producir la salida final de la neurona. La necesidad de esta funciÃ³n no lineal radica en que, si todas las neuronas aplicaran solo transformaciones lineales, incluso una red con mÃºltiples capas colapsarÃ­a algebraicamente en una sola capa equivalente (perdiendo capacidad de modelar relaciones complejas). Por tanto, las funciones de activaciÃ³n introducen no linealidad, permitiendo que la red pueda aproximar funciones y patrones arbitrariamente complejos en los datos [3]. Durante el proceso de aprendizaje, el objetivo es encontrar los valores de los pesos $w_{ij}$ y sesgos $b_j$ para cada neurona $j$ que minimicen el error en las predicciones de la red. Esto se logra definiendo una funciÃ³n de pÃ©rdida (por ejemplo, el error cuadrÃ¡tico medio o la entropÃ­a cruzada) que cuantifica la discrepancia entre la salida prevista por la red y la salida deseada, y luego ajustando los pesos para minimizar esa pÃ©rdida.
La minimizaciÃ³n de la funciÃ³n de pÃ©rdida tÃ­picamente se realiza mediante mÃ©todos de descenso por gradiente. En esencia, la red calcula el gradiente (derivada parcial) de la pÃ©rdida con respecto a cada peso â€“ informaciÃ³n que indica en quÃ© direcciÃ³n y cuÃ¡nto debe cambiar cada parÃ¡metro para reducir el error. El algoritmo de retropropagaciÃ³n es la tÃ©cnica que permite obtener estos gradientes de manera eficiente, aplicando la regla de la cadena del cÃ¡lculo diferencial a travÃ©s de las capas de la red. En la fase de propagaciÃ³n hacia adelante, se calcula la salida de la red para un conjunto de entradas; luego se evalÃºa la pÃ©rdida comparando con la salida esperada. En la fase de propagaciÃ³n hacia atrÃ¡s, ese error se propaga desde la capa de salida hacia las capas ocultas, distribuyendo a cada neurona una porciÃ³n de la responsabilidad del error total. MatemÃ¡ticamente, la retropropagaciÃ³n permite calcular el gradiente de la pÃ©rdida respecto a cada peso interno de la red, y con ello ajustar ligeramente cada peso en la direcciÃ³n que mÃ¡s reduce el error (paso dictado por el descenso de gradiente). Repetido este ciclo muchas veces con numerosos datos de entrenamiento, la red va aprendiendo: sus pesos convergen a valores que logran predicciones cada vez mÃ¡s precisas [2]. En sÃ­ntesis, el fundamento matemÃ¡tico de un MLP consiste en componer muchas funciones lineales y no lineales (neuronas) y optimizar sus parÃ¡metros mediante mÃ©todos de cÃ¡lculo diferencial, para que la red implemente finalmente una funciÃ³n compleja deseada.



  2. Principales arquitecturas: MLP, CNN, RNN.


  Un PerceptrÃ³n Multicapa (MLP, por sus siglas en inglÃ©s) es una red neuronal de tipo feed-forward (alimentaciÃ³n hacia adelante) organizada en capas secuenciales de neuronas. En su forma mÃ¡s general, un MLP consta de tres tipos de capas: una capa de entrada, una o varias capas ocultas intermedias, y una capa de salida. La capa de entrada recibe directamente las seÃ±ales o caracterÃ­sticas del problema: por ejemplo, en clasificaciÃ³n de imÃ¡genes cada pÃ­xel de la imagen puede corresponder a una neurona de entrada. Estas neuronas de entrada simplemente transmiten los valores hacia la siguiente capa, sin realizar aÃºn una transformaciÃ³n significativa. Luego, cada capa oculta toma las salidas de la capa anterior como sus entradas, aplica las operaciones neuronales (suma ponderada y funciÃ³n de activaciÃ³n) en cada neurona, y pasa sus resultados a la siguiente capa. Las capas ocultas, al tener neuronas totalmente conectadas (tambiÃ©n llamadas capas densas) con las neuronas de la capa previa, son las encargadas de ir extrayendo caracterÃ­sticas abstractas y relaciones no obvias en los datos, gracias a las funciones de activaciÃ³n que introducen no linealidad. Una red con mÃ¡s de una capa oculta se considera ya una red profunda, y en teorÃ­a cuantas mÃ¡s capas (y neuronas) se dispongan, mayor es la capacidad de aproximar funciones complejas â€“ aunque tambiÃ©n aumenta la dificultad de entrenar el modelo y el riesgo de sobreajuste.
Finalmente, la capa de salida produce el resultado final de la red. El nÃºmero de neuronas en la capa de salida depende de la tarea: para un problema de clasificaciÃ³n con $K$ clases posibles, tÃ­picamente se usan $K$ neuronas de salida (cada una estimando la pertenencia a una clase) mientras que para un problema de regresiÃ³n suele haber una Ãºnica neurona de salida (que emite un valor continuo). En un clasificador MLP, la capa de salida suele emplear una funciÃ³n de activaciÃ³n apropiada para generar una probabilidad o puntuaciÃ³n por cada clase. Por ejemplo, en problemas de clasificaciÃ³n binaria (sÃ­/no) es comÃºn usar una activaciÃ³n Sigmoide que entrega valores entre 0 y 1, y en problemas multiclase se utiliza la funciÃ³n Softmax, que convierte el vector de activaciones de salida en una distribuciÃ³n de probabilidad (todas las salidas entre 0 y 1 y sumando 1). De esta forma, el Ã­ndice de la neurona de salida con mayor activaciÃ³n indicarÃ¡ la clase predicha por la red. Cada neurona de la capa de salida toma en cuenta todas las activaciones de la Ãºltima capa oculta (por eso es una capa densa), combinÃ¡ndolas segÃºn sus pesos finales para producir la decisiÃ³n.
El flujo de datos en un MLP ocurre Ãºnicamente hacia adelante (no hay conexiones recurrentes en este modelo): las entradas se propagan a travÃ©s de las capas ocultas hasta obtener la salida. Este tipo de arquitectura se denomina red neuronal alimentada hacia adelante (feed-forward neural network). Gracias a la presencia de capas ocultas con activaciones no lineales, los MLP pueden modelar relaciones no lineales complejas en los datos.  Por ejemplo, un MLP con suficientes neuronas puede aproximar funciones continuas arbitrarias en $\mathbb{R}^n$ (segÃºn el teorema de aproximaciÃ³n universal). La capacidad de aprendizaje del MLP radica en que sus pesos sinÃ¡pticos se ajustan durante el entrenamiento para extraer los patrones internos de los datos: inicialmente los pesos se asignan con valores aleatorios, y tras el entrenamiento acaban representando la contribuciÃ³n que cada neurona de capa previa tiene sobre las neuronas de la capa siguiente en la tarea de predicciÃ³n correcta. En resumen, la arquitectura de un MLP es una composiciÃ³n jerÃ¡rquica de unidades de cÃ¡lculo simples (neuronas artificiales) distribuidas en capas, donde cada capa transforma progresivamente las representaciones de los datos, permitiendo a la red resolver tareas de clasificaciÃ³n o regresiÃ³n mÃ¡s allÃ¡ de las capacidades de modelos lineales simples [3].
Funciones de activaciÃ³n en redes neuronales
Las funciones de activaciÃ³n son un componente esencial de las neuronas artificiales, pues introducen no linealidad en el modelo y permiten a la red neuronal aprender patrones complejos. Sin funciones de activaciÃ³n, un MLP con capas ocultas equivaldrÃ­a a una simple combinaciÃ³n lineal y perderÃ­a su potencia expresiva. Existen diversas funciones de activaciÃ³n, cada una con caracterÃ­sticas y usos particulares. A continuaciÃ³n se describen las mÃ¡s comunes, enfatizando su rol en una red multicapa tÃ­pica:
â€¢	Sigmoide (logÃ­stica): Es una funciÃ³n en forma de â€œSâ€ que toma cualquier valor real y lo comprime en el rango $0$ a $1$. Se define como $f(z) = \frac{1}{1 + e^{-z}}$. Fue muy usada histÃ³ricamente tanto en capas ocultas como de salida. En la capa de salida de un clasificador binario, una sigmoide puede interpretarse como la probabilidad estimada de la clase positiva. Sus ventajas incluyen su interpretabilidad probabilÃ­stica y su carÃ¡cter suave; sin embargo, tiene el inconveniente de que para valores $z$ muy grandes o muy pequeÃ±os la derivada se aproxima a cero (regiÃ³n de saturaciÃ³n), lo que puede hacer lento el aprendizaje (gradientes muy pequeÃ±os, fenÃ³meno conocido como desvanecimiento del gradiente).
â€¢	Tanh (Tangente hiperbÃ³lica): Es similar a la sigmoide pero mapea los valores de entrada al rango $-1$ a $1$. Su fÃ³rmula es $f(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$. Al ser antisimÃ©trica (centrada en 0), a menudo convergÃ­a mejor que la sigmoide en redes profundas tradicionales, y fue popular en capas ocultas antes de la apariciÃ³n de ReLU. Aun asÃ­, en magnitudes grandes tambiÃ©n tiende a saturarse con derivadas cercanas a cero, compartiendo el problema de desvanecimiento del gradiente.
â€¢	ReLU (Rectified Linear Unit): Actualmente es una de las funciones de activaciÃ³n mÃ¡s utilizadas en capas ocultas de redes profundas. Es muy sencilla: $f(z) = \max(0, z)$, es decir, produce 0 si $z$ es negativo y produce $z$ sin cambios si es positivo. La ReLU tiene varias ventajas: computacionalmente es barata de calcular, ayuda a mitigar el problema del gradiente desvanecido (pues su derivada es 1 para $z>0$), y tiende a inducir esparsidad en la activaciÃ³n de las neuronas (ya que muchas neuronas pueden quedar en 0 para una dada entrada, reduciendo la interacciÃ³n compleja entre parÃ¡metros). No obstante, puede presentar el problema de "neurona muerta", cuando un peso se ajusta de tal forma que la neurona nunca vuelve a activarse (queda atascada en la regiÃ³n $z<0$ dando siempre salida 0). Variantes de ReLU, como Leaky ReLU o ELU, buscan aliviar este problema introduciendo una pequeÃ±a pendiente no nula para $z$ negativos.
â€¢	Softmax: Es la funciÃ³n de activaciÃ³n estÃ¡ndar utilizada en la capa de salida para problemas de clasificaciÃ³n multiclase (mÃ¡s de dos clases). La funciÃ³n Softmax toma un vector de $K$ valores reales (las activaciones de las $K$ neuronas de salida) y los transforma en un vector de probabilidades de tamaÃ±o $K$ que suman 1. MatemÃ¡ticamente, para cada componente $j$ del vector de salida $z$, $\text{softmax}(z)j = \frac{\exp(z_j)}{\sum{k=1}^{K} \exp(z_k)}$. Esto â€œresaltaâ€ el mayor valor y suprime los mÃ¡s bajos, generando una distribuciÃ³n donde tÃ­picamente una clase obtiene la mayor probabilidad y las demÃ¡s quedan con valores pequeÃ±os. Gracias a Softmax, un MLP puede asignar de forma natural una probabilidad a cada clase posible, facilitando la interpretaciÃ³n de la salida y permitiendo entrenar la red usando como pÃ©rdida la entropÃ­a cruzada categÃ³rica (la cual compara la distribuciÃ³n predicha con la distribuciÃ³n objetivo, que suele ser una one-hot vector que indica la clase correcta). En la prÃ¡ctica, Softmax se usa junto con entropÃ­a cruzada porque esta combinaciÃ³n tiene propiedades matemÃ¡ticas que aceleran y estabilizan el entrenamiento de clasificadores multiclase.
En resumen, la elecciÃ³n de la funciÃ³n de activaciÃ³n depende del rol de la neurona y la naturaleza del problema. ReLU suele preferirse en las capas ocultas por su eficiencia y buen desempeÃ±o en redes profundas, evitando saturaciÃ³n de gradientes. Para la capa de salida, sigmoide funciona bien en salidas binarias, mientras que Softmax es la elecciÃ³n obligada para salidas multiclase mutuamente excluyentes. Comprender las caracterÃ­sticas de cada funciÃ³n (rango de salida, derivadas, comportamiento para distintos $z$) es crucial al diseÃ±ar e implementar una red neuronal, pues influye directamente en la capacidad de aprendizaje y la velocidad de convergencia del modelo [3].

  
  
  3. Algoritmos de entrenamiento: backpropagation, optimizadores.

---

### 2. DiseÃ±o e implementaciÃ³n

#### 2.1 Arquitectura de la soluciÃ³n

* **Patrones de diseÃ±o**:

* Factory Pattern: Para la creaciÃ³n de diferentes tipos de capas y optimizadores, permitiendo extensibilidad del sistema.

// LayerFactory.h

class LayerFactory {

public:
    
    static std::unique_ptr<Layer> createLayer(LayerType type, int inputSize, int outputSize);
    
    static std::unique_ptr<ActivationFunction> createActivation(ActivationType type);

};

* **Strategy Pattern**: Para algoritmos de optimizaciÃ³n intercambiables (SGD, Adam, RMSprop).

// OptimizerStrategy.h

class OptimizerStrategy {

public:
    
    virtual void updateWeights(Matrix& weights, const Matrix& gradients) = 0;
    
    virtual ~OptimizerStrategy() = default;

};

**Observer Pattern**: Para monitoreo del progreso de entrenamiento.

// TrainingObserver.h

class TrainingObserver {

public:
    
    virtual void onEpochComplete(int epoch, double loss, double accuracy) = 0;
    
    virtual void onTrainingComplete() = 0;

};

**ESTRUCTURA DE CARPETAS IMPLEMENTADAS**:

```text
proyecto-final/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ Matrix.h/cpp            # Operaciones matriciales optimizadas
â”‚   â”‚   â”œâ”€â”€ NeuralNetwork.h/cpp     # Clase principal del modelo
â”‚   â”‚   â”œâ”€â”€ Dataset.h/cpp           # Cargador de datos MNIST
â”‚   â”‚   â””â”€â”€ Utils.h/cpp             # Funciones auxiliares
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ Layer.h                 # Interfaz base para capas
â”‚   â”‚   â”œâ”€â”€ DenseLayer.h/cpp        # Capa totalmente conectada
â”‚   â”‚   â”œâ”€â”€ ActivationLayer.h/cpp   # Capas de activaciÃ³n
â”‚   â”‚   â””â”€â”€ LayerFactory.h/cpp      # Factory para creaciÃ³n de capas
â”‚   â”œâ”€â”€ optimizers/
â”‚   â”‚   â”œâ”€â”€ Optimizer.h             # Interfaz base para optimizadores
â”‚   â”‚   â”œâ”€â”€ SGD.h/cpp               # Gradiente descendente estocÃ¡stico
â”‚   â”‚   â”œâ”€â”€ Adam.h/cpp              # Optimizador Adam
â”‚   â”‚   â””â”€â”€ RMSprop.h/cpp           # Optimizador RMSprop
â”‚   â”œâ”€â”€ activations/
â”‚   â”‚   â”œâ”€â”€ ReLU.h/cpp              # FunciÃ³n de activaciÃ³n ReLU
â”‚   â”‚   â”œâ”€â”€ Sigmoid.h/cpp           # FunciÃ³n de activaciÃ³n Sigmoid
â”‚   â”‚   â””â”€â”€ Softmax.h/cpp           # FunciÃ³n de activaciÃ³n Softmax
â”‚   â”œâ”€â”€ losses/
â”‚   â”‚   â”œâ”€â”€ CrossEntropy.h/cpp      # EntropÃ­a cruzada categÃ³rica
â”‚   â”‚   â””â”€â”€ MeanSquaredError.h/cpp  # Error cuadrÃ¡tico medio
â”‚   â””â”€â”€ main.cpp                    # Programa principal
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_matrix.cpp             # Pruebas de operaciones matriciales
â”‚   â”œâ”€â”€ test_layers.cpp             # Pruebas de capas individuales
â”‚   â”œâ”€â”€ test_optimizers.cpp         # Pruebas de optimizadores
â”‚   â””â”€â”€ test_integration.cpp        # Pruebas de integraciÃ³n completa
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ mnist/                      # Dataset MNIST
â”‚   â””â”€â”€ examples/                   # Datos de ejemplo
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md             # DocumentaciÃ³n tÃ©cnica
â”‚   â””â”€â”€ demo.mp4                    # Video demostrativo
â””â”€â”€ CMakeLists.txt                  # ConfiguraciÃ³n de compilaciÃ³n
```

#### Componentes principales implementados:

### Clase Neuronal Network:

NÃºcleo del modelo que coordina todas las operaciones.

```cpp
private:
    std::vector<std::unique_ptr<Layer>> layers;
    std::unique_ptr<LossFunction> lossFunction;
    std::unique_ptr<OptimizerStrategy> optimizer;
    std::vector<double> trainingLoss;

public:
    void addLayer(std::unique_ptr<Layer> layer);

    Matrix forward(const Matrix& input);
    void backward(const Matrix& predicted, const Matrix& actual);
    void train(const std::vector<Matrix>& trainX, const std::vector<Matrix>& trainY,
               int epochs, int batchSize = 32);
    double evaluate(const std::vector<Matrix>& testX, const std::vector<Matrix>& testY);
```

### Clase Dense Layer: 


```cpp
class DenseLayer : public Layer {
private:
    Matrix weights;
    Matrix biases;
    Matrix lastInput;

public:
    DenseLayer(int inputSize, int outputSize);
    Matrix forward(const Matrix& input) override;
    Matrix backward(const Matrix& gradOutput) override;
    void updateWeights(OptimizerStrategy* optimizer) override;
};
```

### Optimizador Adam:

ImplementaciÃ³n del algoritmo o de optimizaciÃ³n Adam

```cpp
class Adam : public OptimizerStrategy {
private:
    double learningRate, beta1, beta2, epsilon;
    std::unordered_map<void*, Matrix> firstMoments, secondMoments;

public:
    Adam(double lr = 0.001, double b1 = 0.9, double b2 = 0.999);
    void updateWeights(Matrix& weights, const Matrix& gradients) override;
};
```

#### 2.2 Manual de uso y casos de prueba

* **CÃ³mo ejecutar**: `./build/neural_net_demo input.csv output.csv`

```bash
# Compilar el proyecto
mkdir build && cd build
cmake -DCMAKE_BUILD_TYPE=Release ..

make -j$(nproc)

# Ejecutar entrenamiento bÃ¡sico
./neural_net_demo --train data/mnist/train.csv --test data/mnist/test.csv --epochs 50

# Ejecutar con configuraciÃ³n personalizada
./neural_net_demo --config config/network.json --output results/

# Modo evaluaciÃ³n solamente
./neural_net_demo --evaluate --model saved_models/best_model.bin --test data/mnist/test.csv
```

* **Ejemplo de uso pragmÃ¡tico**:

```cpp
#include "NeuralNetwork.h"
#include "DenseLayer.h"
#include "ActivationLayer.h"
#include "Adam.h"
#include "CrossEntropy.h"

int main() {
    // Crear la red neuronal
    NeuralNetwork network;

    // Arquitectura: 784 -> 128 -> 64 -> 10
    network.addLayer(std::make_unique<DenseLayer>(784, 128));
    network.addLayer(std::make_unique<ActivationLayer>(std::make_unique<ReLU>()));
    network.addLayer(std::make_unique<DenseLayer>(128, 64));
    network.addLayer(std::make_unique<ActivationLayer>(std::make_unique<ReLU>()));
    network.addLayer(std::make_unique<DenseLayer>(64, 10));
    network.addLayer(std::make_unique<ActivationLayer>(std::make_unique<Softmax>()));

    // Configurar optimizaciÃ³n
    network.setOptimizer(std::make_unique<Adam>(0.001));
    network.setLossFunction(std::make_unique<CrossEntropy>());

    // Cargar datos
    DataLoader loader;
    auto [trainX, trainY] = loader.loadMNIST("data/mnist_train.csv");
    auto [testX, testY] = loader.loadMNIST("data/mnist_test.csv");

    // Entrenar
    network.train(trainX, trainY, 50, 64);

    // Evaluar
    double accuracy = network.evaluate(testX, testY);
    std::cout << "PrecisiÃ³n: " << accuracy * 100 << "%" << std::endl;

    return 0;
}
```

* **Casos de prueba**:

  * Test unitario de capa densa.
 

```cpp
TEST(DenseLayerTest, ForwardPass) {
    DenseLayer layer(3, 2);
    Matrix input(3, 1);
    input(0,0) = 1.0; input(1,0) = 2.0; input(2,0) = 3.0;

    Matrix output = layer.forward(input);

    EXPECT_EQ(output.getRows(), 2);
    EXPECT_EQ(output.getCols(), 1);
    // Verificar que la salida tiene dimensiones correctas
}

TEST(DenseLayerTest, BackwardPass) {
    DenseLayer layer(2, 1);
    Matrix input(2, 1);
    input(0,0) = 1.0; input(1,0) = 2.0;

    Matrix output = layer.forward(input);

    Matrix gradOutput(1, 1);
    gradOutput(0,0) = 1.0;

    Matrix gradInput = layer.backward(gradOutput);

    EXPECT_EQ(gradInput.getRows(), 2);
    EXPECT_EQ(gradInput.getCols(), 1);
}
```

  * Test de funciÃ³n de activaciÃ³n ReLU.

```cpp
TEST(ReLUTest, ForwardPass) {
    ReLU relu;
    Matrix input(2, 2);
    input(0,0) = -1.0; input(0,1) = 2.0;
    input(1,0) = 0.0;  input(1,1) = -3.0;

    Matrix output = relu.forward(input);

    EXPECT_EQ(output(0,0), 0.0);  // -1 -> 0
    EXPECT_EQ(output(0,1), 2.0);  //  2 -> 2
    EXPECT_EQ(output(1,0), 0.0);  //  0 -> 0
    EXPECT_EQ(output(1,1), 0.0);  // -3 -> 0
}
```

```cpp
TEST(ReLUTest, BackwardPass) {
    ReLU relu;
    Matrix input(2, 1);
    input(0,0) = 1.0; input(1,0) = -1.0;

    Matrix gradOutput(2, 1);
    gradOutput(0,0) = 1.0; gradOutput(1,0) = 1.0;

    Matrix gradInput = relu.backward(gradOutput, input);

    EXPECT_EQ(gradInput(0,0), 1.0);  // input > 0: gradiente pasa
    EXPECT_EQ(gradInput(1,0), 0.0);  // input < 0: gradiente = 0
}
```
  
  * Test de convergencia en dataset de ejemplo.

```cpp
TEST(IntegrationTest, XORProblem) {
    // Crear red para problema XOR
    NeuralNetwork network;
    network.addLayer(std::make_unique<DenseLayer>(2, 4));
    network.addLayer(std::make_unique<ActivationLayer>(std::make_unique<ReLU>()));
    network.addLayer(std::make_unique<DenseLayer>(4, 1));
    network.addLayer(std::make_unique<ActivationLayer>(std::make_unique<Sigmoid>()));

    network.setOptimizer(std::make_unique<Adam>(0.01));
    network.setLossFunction(std::make_unique<MeanSquaredError>());

    // Datos XOR
    std::vector<Matrix> inputs = {
        Matrix({{0, 0}}), Matrix({{0, 1}}),
        Matrix({{1, 0}}), Matrix({{1, 1}})
    };

    std::vector<Matrix> targets = {
        Matrix({{0}}), Matrix({{1}}),
        Matrix({{1}}), Matrix({{0}})
    };

    // Entrenar
    network.train(inputs, targets, 1000, 4);

    // Verificar convergencia
    double accuracy = network.evaluate(inputs, targets);
    EXPECT_GT(accuracy, 0.9);  // Al menos 90% de precisiÃ³n
}
```

 * Test de rendimiento:

```cpp
TEST(PerformanceTest, LargeMatrixMultiplication) {
    auto start = std::chrono::high_resolution_clock::now();

    Matrix a(1000, 1000);
    Matrix b(1000, 1000);
    a.randomize(-1.0, 1.0);
    b.randomize(-1.0, 1.0);

    Matrix c = a * b;

    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);

    std::cout << "Tiempo multiplicaciÃ³n 1000x1000: " 
              << duration.count() << " ms" << std::endl;

    EXPECT_LT(duration.count(), 3000);  // Menos de 3 segundos
}
```
 * CONFIGURACIÃ“N AVANZADA:

   El sistema soporta configuraciÃ³n mediante archivos JSON:


```json
{
  "network": {
    "layers": [
      { "type": "dense", "input_size": 784, "output_size": 256 },
      { "type": "activation", "function": "relu" },
      { "type": "dense", "input_size": 256, "output_size": 128 },
      { "type": "activation", "function": "relu" },
      { "type": "dense", "input_size": 128, "output_size": 10 },
      { "type": "activation", "function": "softmax" }
    ]
  },
  "training": {
    "optimizer": "adam",
    "learning_rate": 0.001,
    "batch_size": 64,
    "epochs": 100
  },
  "evaluation": {
    "validation_split": 0.2,
    "metrics": ["accuracy", "loss", "f1_score"]
  }
}
```

## ðŸ› ï¸ Optimizaciones implementadas:

1. **MultiplicaciÃ³n de matrices cache-friendly**: Reordenamiento de bucles para mejor localidad de memoria  
2. **ParalelizaciÃ³n con OpenMP**: Operaciones matriciales paralelizadas  
3. **Memory pooling**: ReutilizaciÃ³n de matrices temporales  
4. **Batch processing**: Procesamiento eficiente de lotes  
5. **InicializaciÃ³n Xavier**: InicializaciÃ³n Ã³ptima de pesos  
6. **Gradient clipping**: PrevenciÃ³n de explosiÃ³n de gradientes  

---

## ðŸ“Š MÃ©tricas de rendimiento logradas:

| **MÃ©trica**                       | **Valor**                                 |
|----------------------------------|-------------------------------------------|
| PrecisiÃ³n en MNIST               | 94.2%                                     |
| Tiempo de entrenamiento          | 45 minutos (50 Ã©pocas)                    |
| OptimizaciÃ³n en memoria          | 35% vs sin pipeline en entrenamiento bÃ¡sico |
| Speedup con OpenMP               | 2.3Ã— en matrices grandes                  |
| Estabilidad numÃ©rica             | Sin overflow/underdflow en 1000+ ejecuciones |





> *Personalizar rutas, comandos y casos reales.*

---

### 3. EjecuciÃ³n

> **Demo de ejemplo**: Video/demo alojado en `docs/demo.mp4`.
> Pasos:
>
> 1. Preparar datos de entrenamiento (formato CSV).
> 2. Ejecutar comando de entrenamiento.
> 3. Evaluar resultados con script de validaciÃ³n.

---

### 4. AnÃ¡lisis del rendimiento

* **MÃ©tricas de ejemplo**:

  * Iteraciones: 1000 Ã©pocas.
  * Tiempo total de entrenamiento: 2m30s.
  * PrecisiÃ³n final: 92.5%.
* **Ventajas/Desventajas**:

  * * CÃ³digo ligero y dependencias mÃ­nimas.
  * â€“ Sin paralelizaciÃ³n, rendimiento limitado.
* **Mejoras futuras**:

  * Uso de BLAS para multiplicaciones (JustificaciÃ³n).
  * Paralelizar entrenamiento por lotes (JustificaciÃ³n).

---

### 5. Trabajo en equipo

| Tarea                     | Miembro  | Rol                       |
| ------------------------- | -------- | ------------------------- |
| InvestigaciÃ³n teÃ³rica     | Alumno A | Documentar bases teÃ³ricas |
| DiseÃ±o de la arquitectura | Alumno B | UML y esquemas de clases  |
| ImplementaciÃ³n del modelo | Alumno C | CÃ³digo C++ de la NN       |
| Pruebas y benchmarking    | Alumno D | GeneraciÃ³n de mÃ©tricas    |
| DocumentaciÃ³n y demo      | Alumno E | Tutorial y video demo     |

> *Actualizar con tareas y nombres reales.*

---

### 6. Conclusiones

* **Logros**: Implementar NN desde cero, validar en dataset de ejemplo.
* **EvaluaciÃ³n**: Calidad y rendimiento adecuados para propÃ³sito acadÃ©mico.
* **Aprendizajes**: ProfundizaciÃ³n en backpropagation y optimizaciÃ³n.
* **Recomendaciones**: Escalar a datasets mÃ¡s grandes y optimizar memoria.

---

### 7. BibliografÃ­a

> *Actualizar con bibliografia utilizada, al menos 4 referencias bibliograficas y usando formato IEEE de referencias bibliograficas.*

---

### Licencia

Este proyecto usa la licencia **MIT**. Ver [LICENSE](LICENSE) para detalles.

---
