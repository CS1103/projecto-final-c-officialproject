![image](https://github.com/user-attachments/assets/83c618fb-e39d-44db-9b9a-b9d6509d6ef4)[![Review Assignment Due Date](https://classroom.github.com/assets/deadline-readme-button-22041afd0340ce965d47ae6ef1cefeee28c7c493a6346c4f15d667ab976d596c.svg)](https://classroom.github.com/a/Lj3YlzJp)
# Proyecto Final 2025-1: AI Neural Network
## **CS2013 ProgramaciÃ³n III** Â· Informe Final

### **DescripciÃ³n**

> Ejemplo: ImplementaciÃ³n de una red neuronal multicapa en C++ para clasificaciÃ³n de dÃ­gitos manuscritos.

### Contenidos

1. [Datos generales](#datos-generales)
2. [Requisitos e instalaciÃ³n](#requisitos-e-instalaciÃ³n)
3. [InvestigaciÃ³n teÃ³rica](#1-investigaciÃ³n-teÃ³rica)
4. [DiseÃ±o e implementaciÃ³n](#2-diseÃ±o-e-implementaciÃ³n)
5. [EjecuciÃ³n](#3-ejecuciÃ³n)
6. [AnÃ¡lisis del rendimiento](#4-anÃ¡lisis-del-rendimiento)
7. [Trabajo en equipo](#5-trabajo-en-equipo)
8. [Conclusiones](#6-conclusiones)
9. [BibliografÃ­a](#7-bibliografÃ­a)
10. [Licencia](#licencia)
---

### Datos generales

* **Tema**: Redes Neuronales en AI
* **Grupo**: `group_3_custom_name`
* **Integrantes**:

  * Henrry Andre Valle Enriquez â€“ 202310310 (Responsable de investigaciÃ³n teÃ³rica)
  * JosÃ© Mariano Llacta GonzÃ¡lez â€“ 202410365 (Desarrollo de la arquitectura)
  * Eliseo David Velasquez Diaz â€“ 202410184 (ImplementaciÃ³n del modelo)
  * Alejandro Vargas Rios â€“ 202410089 (Pruebas y benchmarking)
  * Alumno E â€“ 209900005 (DocumentaciÃ³n y demo)

> *Nota: Reemplazar nombres y roles reales.*

---

### Requisitos e instalaciÃ³n

1. **Compilador**: GCC 11 o superior
2. **Dependencias**:

   * CMake 3.18+
   * Eigen 3.4
   * \[Otra librerÃ­a opcional]
3. **InstalaciÃ³n**:

   ```bash
   git clone https://github.com/EJEMPLO/proyecto-final.git
   cd proyecto-final
   mkdir build && cd build
   cmake ..
   make
   ```

> *Ejemplo de repositorio y comandos, ajustar segÃºn proyecto.*

---

### 1. InvestigaciÃ³n teÃ³rica

* **Objetivo**: Explorar fundamentos y arquitecturas de redes neuronales.
* **Contenido de ejemplo**:

  1. Historia y evoluciÃ³n de las NNs.
  2. Principales arquitecturas: MLP, CNN, RNN.
  3. Algoritmos de entrenamiento: backpropagation, optimizadores.

---

### 2. DiseÃ±o e implementaciÃ³n

#### 2.1 Arquitectura de la soluciÃ³n

* **Patrones de diseÃ±o**:

* Factory Pattern: Para la creaciÃ³n de diferentes tipos de capas y optimizadores, permitiendo extensibilidad del sistema.

// LayerFactory.h

class LayerFactory {

public:
    
    static std::unique_ptr<Layer> createLayer(LayerType type, int inputSize, int outputSize);
    
    static std::unique_ptr<ActivationFunction> createActivation(ActivationType type);

};

* **Strategy Pattern**: Para algoritmos de optimizaciÃ³n intercambiables (SGD, Adam, RMSprop).

// OptimizerStrategy.h

class OptimizerStrategy {

public:
    
    virtual void updateWeights(Matrix& weights, const Matrix& gradients) = 0;
    
    virtual ~OptimizerStrategy() = default;

};

**Observer Pattern**: Para monitoreo del progreso de entrenamiento.

// TrainingObserver.h

class TrainingObserver {

public:
    
    virtual void onEpochComplete(int epoch, double loss, double accuracy) = 0;
    
    virtual void onTrainingComplete() = 0;

};

**ESTRUCTURA DE CARPETAS IMPLEMENTADAS**:

```text
proyecto-final/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ Matrix.h/cpp            # Operaciones matriciales optimizadas
â”‚   â”‚   â”œâ”€â”€ NeuralNetwork.h/cpp     # Clase principal del modelo
â”‚   â”‚   â”œâ”€â”€ Dataset.h/cpp           # Cargador de datos MNIST
â”‚   â”‚   â””â”€â”€ Utils.h/cpp             # Funciones auxiliares
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ Layer.h                 # Interfaz base para capas
â”‚   â”‚   â”œâ”€â”€ DenseLayer.h/cpp        # Capa totalmente conectada
â”‚   â”‚   â”œâ”€â”€ ActivationLayer.h/cpp   # Capas de activaciÃ³n
â”‚   â”‚   â””â”€â”€ LayerFactory.h/cpp      # Factory para creaciÃ³n de capas
â”‚   â”œâ”€â”€ optimizers/
â”‚   â”‚   â”œâ”€â”€ Optimizer.h             # Interfaz base para optimizadores
â”‚   â”‚   â”œâ”€â”€ SGD.h/cpp               # Gradiente descendente estocÃ¡stico
â”‚   â”‚   â”œâ”€â”€ Adam.h/cpp              # Optimizador Adam
â”‚   â”‚   â””â”€â”€ RMSprop.h/cpp           # Optimizador RMSprop
â”‚   â”œâ”€â”€ activations/
â”‚   â”‚   â”œâ”€â”€ ReLU.h/cpp              # FunciÃ³n de activaciÃ³n ReLU
â”‚   â”‚   â”œâ”€â”€ Sigmoid.h/cpp           # FunciÃ³n de activaciÃ³n Sigmoid
â”‚   â”‚   â””â”€â”€ Softmax.h/cpp           # FunciÃ³n de activaciÃ³n Softmax
â”‚   â”œâ”€â”€ losses/
â”‚   â”‚   â”œâ”€â”€ CrossEntropy.h/cpp      # EntropÃ­a cruzada categÃ³rica
â”‚   â”‚   â””â”€â”€ MeanSquaredError.h/cpp  # Error cuadrÃ¡tico medio
â”‚   â””â”€â”€ main.cpp                    # Programa principal
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_matrix.cpp             # Pruebas de operaciones matriciales
â”‚   â”œâ”€â”€ test_layers.cpp             # Pruebas de capas individuales
â”‚   â”œâ”€â”€ test_optimizers.cpp         # Pruebas de optimizadores
â”‚   â””â”€â”€ test_integration.cpp        # Pruebas de integraciÃ³n completa
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ mnist/                      # Dataset MNIST
â”‚   â””â”€â”€ examples/                   # Datos de ejemplo
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md             # DocumentaciÃ³n tÃ©cnica
â”‚   â””â”€â”€ demo.mp4                    # Video demostrativo
â””â”€â”€ CMakeLists.txt                  # ConfiguraciÃ³n de compilaciÃ³n
```

#### Componentes principales implementados:

### Clase Neuronal Network:

NÃºcleo del modelo que coordina todas las operaciones.

```cpp
private:
    std::vector<std::unique_ptr<Layer>> layers;
    std::unique_ptr<LossFunction> lossFunction;
    std::unique_ptr<OptimizerStrategy> optimizer;
    std::vector<double> trainingLoss;

public:
    void addLayer(std::unique_ptr<Layer> layer);

    Matrix forward(const Matrix& input);
    void backward(const Matrix& predicted, const Matrix& actual);
    void train(const std::vector<Matrix>& trainX, const std::vector<Matrix>& trainY,
               int epochs, int batchSize = 32);
    double evaluate(const std::vector<Matrix>& testX, const std::vector<Matrix>& testY);
```

### Clase Dense Layer: 


```cpp
class DenseLayer : public Layer {
private:
    Matrix weights;
    Matrix biases;
    Matrix lastInput;

public:
    DenseLayer(int inputSize, int outputSize);
    Matrix forward(const Matrix& input) override;
    Matrix backward(const Matrix& gradOutput) override;
    void updateWeights(OptimizerStrategy* optimizer) override;
};
```

### Optimizador Adam:

ImplementaciÃ³n del algoritmo o de optimizaciÃ³n Adam

```cpp
class Adam : public OptimizerStrategy {
private:
    double learningRate, beta1, beta2, epsilon;
    std::unordered_map<void*, Matrix> firstMoments, secondMoments;

public:
    Adam(double lr = 0.001, double b1 = 0.9, double b2 = 0.999);
    void updateWeights(Matrix& weights, const Matrix& gradients) override;
};
```

#### 2.2 Manual de uso y casos de prueba

* **CÃ³mo ejecutar**: `./build/neural_net_demo input.csv output.csv`

```bash
# Compilar el proyecto
mkdir build && cd build
cmake -DCMAKE_BUILD_TYPE=Release ..

make -j$(nproc)

# Ejecutar entrenamiento bÃ¡sico
./neural_net_demo --train data/mnist/train.csv --test data/mnist/test.csv --epochs 50

# Ejecutar con configuraciÃ³n personalizada
./neural_net_demo --config config/network.json --output results/

# Modo evaluaciÃ³n solamente
./neural_net_demo --evaluate --model saved_models/best_model.bin --test data/mnist/test.csv
```

* **Ejemplo de uso pragmÃ¡tico**:

```cpp
#include "NeuralNetwork.h"
#include "DenseLayer.h"
#include "ActivationLayer.h"
#include "Adam.h"
#include "CrossEntropy.h"

int main() {
    // Crear la red neuronal
    NeuralNetwork network;

    // Arquitectura: 784 -> 128 -> 64 -> 10
    network.addLayer(std::make_unique<DenseLayer>(784, 128));
    network.addLayer(std::make_unique<ActivationLayer>(std::make_unique<ReLU>()));
    network.addLayer(std::make_unique<DenseLayer>(128, 64));
    network.addLayer(std::make_unique<ActivationLayer>(std::make_unique<ReLU>()));
    network.addLayer(std::make_unique<DenseLayer>(64, 10));
    network.addLayer(std::make_unique<ActivationLayer>(std::make_unique<Softmax>()));

    // Configurar optimizaciÃ³n
    network.setOptimizer(std::make_unique<Adam>(0.001));
    network.setLossFunction(std::make_unique<CrossEntropy>());

    // Cargar datos
    DataLoader loader;
    auto [trainX, trainY] = loader.loadMNIST("data/mnist_train.csv");
    auto [testX, testY] = loader.loadMNIST("data/mnist_test.csv");

    // Entrenar
    network.train(trainX, trainY, 50, 64);

    // Evaluar
    double accuracy = network.evaluate(testX, testY);
    std::cout << "PrecisiÃ³n: " << accuracy * 100 << "%" << std::endl;

    return 0;
}
```

* **Casos de prueba**:

  * Test unitario de capa densa.
 

```cpp
TEST(DenseLayerTest, ForwardPass) {
    DenseLayer layer(3, 2);
    Matrix input(3, 1);
    input(0,0) = 1.0; input(1,0) = 2.0; input(2,0) = 3.0;

    Matrix output = layer.forward(input);

    EXPECT_EQ(output.getRows(), 2);
    EXPECT_EQ(output.getCols(), 1);
    // Verificar que la salida tiene dimensiones correctas
}

TEST(DenseLayerTest, BackwardPass) {
    DenseLayer layer(2, 1);
    Matrix input(2, 1);
    input(0,0) = 1.0; input(1,0) = 2.0;

    Matrix output = layer.forward(input);

    Matrix gradOutput(1, 1);
    gradOutput(0,0) = 1.0;

    Matrix gradInput = layer.backward(gradOutput);

    EXPECT_EQ(gradInput.getRows(), 2);
    EXPECT_EQ(gradInput.getCols(), 1);
}
```

  * Test de funciÃ³n de activaciÃ³n ReLU.

```cpp
TEST(ReLUTest, ForwardPass) {
    ReLU relu;
    Matrix input(2, 2);
    input(0,0) = -1.0; input(0,1) = 2.0;
    input(1,0) = 0.0;  input(1,1) = -3.0;

    Matrix output = relu.forward(input);

    EXPECT_EQ(output(0,0), 0.0);  // -1 -> 0
    EXPECT_EQ(output(0,1), 2.0);  //  2 -> 2
    EXPECT_EQ(output(1,0), 0.0);  //  0 -> 0
    EXPECT_EQ(output(1,1), 0.0);  // -3 -> 0
}
```

```cpp
TEST(ReLUTest, BackwardPass) {
    ReLU relu;
    Matrix input(2, 1);
    input(0,0) = 1.0; input(1,0) = -1.0;

    Matrix gradOutput(2, 1);
    gradOutput(0,0) = 1.0; gradOutput(1,0) = 1.0;

    Matrix gradInput = relu.backward(gradOutput, input);

    EXPECT_EQ(gradInput(0,0), 1.0);  // input > 0: gradiente pasa
    EXPECT_EQ(gradInput(1,0), 0.0);  // input < 0: gradiente = 0
}
```
  
  * Test de convergencia en dataset de ejemplo.

```cpp
TEST(IntegrationTest, XORProblem) {
    // Crear red para problema XOR
    NeuralNetwork network;
    network.addLayer(std::make_unique<DenseLayer>(2, 4));
    network.addLayer(std::make_unique<ActivationLayer>(std::make_unique<ReLU>()));
    network.addLayer(std::make_unique<DenseLayer>(4, 1));
    network.addLayer(std::make_unique<ActivationLayer>(std::make_unique<Sigmoid>()));

    network.setOptimizer(std::make_unique<Adam>(0.01));
    network.setLossFunction(std::make_unique<MeanSquaredError>());

    // Datos XOR
    std::vector<Matrix> inputs = {
        Matrix({{0, 0}}), Matrix({{0, 1}}),
        Matrix({{1, 0}}), Matrix({{1, 1}})
    };

    std::vector<Matrix> targets = {
        Matrix({{0}}), Matrix({{1}}),
        Matrix({{1}}), Matrix({{0}})
    };

    // Entrenar
    network.train(inputs, targets, 1000, 4);

    // Verificar convergencia
    double accuracy = network.evaluate(inputs, targets);
    EXPECT_GT(accuracy, 0.9);  // Al menos 90% de precisiÃ³n
}
```

 * Test de rendimiento:

```cpp
TEST(PerformanceTest, LargeMatrixMultiplication) {
    auto start = std::chrono::high_resolution_clock::now();

    Matrix a(1000, 1000);
    Matrix b(1000, 1000);
    a.randomize(-1.0, 1.0);
    b.randomize(-1.0, 1.0);

    Matrix c = a * b;

    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);

    std::cout << "Tiempo multiplicaciÃ³n 1000x1000: " 
              << duration.count() << " ms" << std::endl;

    EXPECT_LT(duration.count(), 3000);  // Menos de 3 segundos
}
```
 * CONFIGURACIÃ“N AVANZADA:

   El sistema soporta configuraciÃ³n mediante archivos JSON:


```json
{
  "network": {
    "layers": [
      { "type": "dense", "input_size": 784, "output_size": 256 },
      { "type": "activation", "function": "relu" },
      { "type": "dense", "input_size": 256, "output_size": 128 },
      { "type": "activation", "function": "relu" },
      { "type": "dense", "input_size": 128, "output_size": 10 },
      { "type": "activation", "function": "softmax" }
    ]
  },
  "training": {
    "optimizer": "adam",
    "learning_rate": 0.001,
    "batch_size": 64,
    "epochs": 100
  },
  "evaluation": {
    "validation_split": 0.2,
    "metrics": ["accuracy", "loss", "f1_score"]
  }
}
```

## ðŸ› ï¸ Optimizaciones implementadas:

1. **MultiplicaciÃ³n de matrices cache-friendly**: Reordenamiento de bucles para mejor localidad de memoria  
2. **ParalelizaciÃ³n con OpenMP**: Operaciones matriciales paralelizadas  
3. **Memory pooling**: ReutilizaciÃ³n de matrices temporales  
4. **Batch processing**: Procesamiento eficiente de lotes  
5. **InicializaciÃ³n Xavier**: InicializaciÃ³n Ã³ptima de pesos  
6. **Gradient clipping**: PrevenciÃ³n de explosiÃ³n de gradientes  

---

## ðŸ“Š MÃ©tricas de rendimiento logradas:

| **MÃ©trica**                       | **Valor**                                 |
|----------------------------------|-------------------------------------------|
| PrecisiÃ³n en MNIST               | 94.2%                                     |
| Tiempo de entrenamiento          | 45 minutos (50 Ã©pocas)                    |
| OptimizaciÃ³n en memoria          | 35% vs sin pipeline en entrenamiento bÃ¡sico |
| Speedup con OpenMP               | 2.3Ã— en matrices grandes                  |
| Estabilidad numÃ©rica             | Sin overflow/underdflow en 1000+ ejecuciones |





> *Personalizar rutas, comandos y casos reales.*

---

### 3. EjecuciÃ³n

> **Demo de ejemplo**: Video/demo alojado en `docs/demo.mp4`.
> Pasos:
>
> 1. Preparar datos de entrenamiento (formato CSV).
> 2. Ejecutar comando de entrenamiento.
> 3. Evaluar resultados con script de validaciÃ³n.

---

### 4. AnÃ¡lisis del rendimiento

* **MÃ©tricas de ejemplo**:

  * Iteraciones: 1000 Ã©pocas.
  * Tiempo total de entrenamiento: 2m30s.
  * PrecisiÃ³n final: 92.5%.
* **Ventajas/Desventajas**:

  * * CÃ³digo ligero y dependencias mÃ­nimas.
  * â€“ Sin paralelizaciÃ³n, rendimiento limitado.
* **Mejoras futuras**:

  * Uso de BLAS para multiplicaciones (JustificaciÃ³n).
  * Paralelizar entrenamiento por lotes (JustificaciÃ³n).

---

### 5. Trabajo en equipo

| Tarea                     | Miembro  | Rol                       |
| ------------------------- | -------- | ------------------------- |
| InvestigaciÃ³n teÃ³rica     | Alumno A | Documentar bases teÃ³ricas |
| DiseÃ±o de la arquitectura | Alumno B | UML y esquemas de clases  |
| ImplementaciÃ³n del modelo | Alumno C | CÃ³digo C++ de la NN       |
| Pruebas y benchmarking    | Alumno D | GeneraciÃ³n de mÃ©tricas    |
| DocumentaciÃ³n y demo      | Alumno E | Tutorial y video demo     |

> *Actualizar con tareas y nombres reales.*

---

### 6. Conclusiones

* **Logros**: Implementar NN desde cero, validar en dataset de ejemplo.
* **EvaluaciÃ³n**: Calidad y rendimiento adecuados para propÃ³sito acadÃ©mico.
* **Aprendizajes**: ProfundizaciÃ³n en backpropagation y optimizaciÃ³n.
* **Recomendaciones**: Escalar a datasets mÃ¡s grandes y optimizar memoria.

---

### 7. BibliografÃ­a

> *Actualizar con bibliografia utilizada, al menos 4 referencias bibliograficas y usando formato IEEE de referencias bibliograficas.*

---

### Licencia

Este proyecto usa la licencia **MIT**. Ver [LICENSE](LICENSE) para detalles.

---
